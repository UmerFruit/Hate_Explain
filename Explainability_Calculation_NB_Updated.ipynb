{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce6b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import more_itertools as mit\n",
    "\n",
    "# Import preprocessing utilities\n",
    "from Preprocess.dataCollect import get_annotated_data\n",
    "from Preprocess.spanMatcher import returnMask\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561cb7e",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6099bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration for 3-class (hatespeech, offensive, normal)\n",
    "dict_data_folder = {\n",
    "    '2': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes_two.npy'},\n",
    "    '3': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes.npy'}\n",
    "}\n",
    "\n",
    "params = {}\n",
    "params['num_classes'] = 3  # hatespeech, offensive, normal\n",
    "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "# Load dataset\n",
    "data_all_labelled = get_annotated_data(params)\n",
    "print(f\"Loaded {len(data_all_labelled)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataset\n",
    "data_all_labelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(data_all_labelled['final_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f16135",
   "metadata": {},
   "source": [
    "## 2. Configure Tokenization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and attention configuration\n",
    "params_data = {\n",
    "    'include_special': False,      # Include special tokens in attention\n",
    "    'bert_tokens': False,          # Use BERT tokenizer (set True for BERT models)\n",
    "    'type_attention': 'softmax',   # Attention type\n",
    "    'set_decay': 0.1,              # Decay parameter\n",
    "    'majority': 2,                 # Majority threshold for rationales\n",
    "    'max_length': 128,             # Maximum sequence length\n",
    "    'variance': 10,                # Variance parameter\n",
    "    'window': 4,                   # Window size\n",
    "    'alpha': 0.5,                  # Alpha parameter\n",
    "    'p_value': 0.8,                # P-value threshold\n",
    "    'method': 'additive',          # Attention combination method\n",
    "    'decay': False,                # Use decay\n",
    "    'normalized': False,           # Normalize attention\n",
    "    'not_recollect': True,         # Don't recollect data\n",
    "}\n",
    "\n",
    "# Initialize tokenizer\n",
    "if params_data['bert_tokens']:\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
    "else:\n",
    "    print('Using standard (non-BERT) tokenizer...')\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec68a13",
   "metadata": {},
   "source": [
    "## 3. Extract Rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9798dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(data, params_data, tokenizer):\n",
    "    \"\"\"\n",
    "    Process dataset to extract token-wise rationales.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with annotated data\n",
    "        params_data: Configuration parameters\n",
    "        tokenizer: BertTokenizer or None\n",
    "    \n",
    "    Returns:\n",
    "        List of [post_id, annotation, tokens, attention_masks, annotation_list]\n",
    "    \"\"\"\n",
    "    final_output = []\n",
    "    print(f'Processing {len(data)} samples...')\n",
    "    \n",
    "    for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        annotation = row['final_label']\n",
    "        post_id = row['post_id']\n",
    "        annotation_list = [row['label1'], row['label2'], row['label3']]\n",
    "        \n",
    "        # Skip undecided samples\n",
    "        if annotation != 'undecided':\n",
    "            tokens_all, attention_masks = returnMask(row, params_data, tokenizer)\n",
    "            final_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training data\n",
    "training_data = get_training_data(data_all_labelled, params_data, tokenizer)\n",
    "print(f\"\\nProcessed {len(training_data)} valid samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d4aea",
   "metadata": {},
   "source": [
    "## 4. Convert to ERASER Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6450f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ranges(iterable):\n",
    "    \"\"\"\n",
    "    Find consecutive ranges in a sorted iterable.\n",
    "    \n",
    "    Args:\n",
    "        iterable: Sorted list of integers\n",
    "    \n",
    "    Yields:\n",
    "        Single integers or (start, end) tuples for consecutive ranges\n",
    "    \"\"\"\n",
    "    for group in mit.consecutive_groups(iterable):\n",
    "        group = list(group)\n",
    "        if len(group) == 1:\n",
    "            yield group[0]\n",
    "        else:\n",
    "            yield group[0], group[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidence(post_id, anno_text, explanations):\n",
    "    \"\"\"\n",
    "    Convert binary explanations to ERASER evidence format.\n",
    "    \n",
    "    Args:\n",
    "        post_id: Document ID\n",
    "        anno_text: List of tokens\n",
    "        explanations: Binary list (1 = rationale token)\n",
    "    \n",
    "    Returns:\n",
    "        List of evidence dictionaries with spans\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    \n",
    "    # Find indices where explanation = 1\n",
    "    indexes = sorted([i for i, val in enumerate(explanations) if val == 1])\n",
    "    \n",
    "    # Convert to spans\n",
    "    span_list = list(find_ranges(indexes))\n",
    "    \n",
    "    for span in span_list:\n",
    "        if isinstance(span, int):\n",
    "            start, end = span, span + 1\n",
    "        elif len(span) == 2:\n",
    "            start, end = span[0], span[1] + 1\n",
    "        else:\n",
    "            print(f'Error processing span: {span}')\n",
    "            continue\n",
    "        \n",
    "        output.append({\n",
    "            \"docid\": post_id,\n",
    "            \"end_sentence\": -1,\n",
    "            \"end_token\": end,\n",
    "            \"start_sentence\": -1,\n",
    "            \"start_token\": start,\n",
    "            \"text\": ' '.join([str(t) for t in anno_text[start:end]])\n",
    "        })\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7951a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):\n",
    "    \"\"\"\n",
    "    Convert dataset to ERASER benchmark format.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Processed training data\n",
    "        method: Rationale combination method ('union', 'intersection')\n",
    "        save_split: Whether to save train/val/test splits\n",
    "        save_path: Directory to save files\n",
    "        id_division: Dict with train/val/test post IDs\n",
    "    \n",
    "    Returns:\n",
    "        List of ERASER-formatted documents\n",
    "    \"\"\"\n",
    "    final_output = []\n",
    "    \n",
    "    if save_split:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_path, 'docs'), exist_ok=True)\n",
    "        \n",
    "        train_fp = open(os.path.join(save_path, 'train.jsonl'), 'w')\n",
    "        val_fp = open(os.path.join(save_path, 'val.jsonl'), 'w')\n",
    "        test_fp = open(os.path.join(save_path, 'test.jsonl'), 'w')\n",
    "    \n",
    "    for row in dataset:\n",
    "        post_id = row[0]\n",
    "        post_class = row[1]\n",
    "        anno_text_list = row[2]\n",
    "        \n",
    "        # Skip 'normal' class (no rationales expected)\n",
    "        if post_class == 'normal':\n",
    "            continue\n",
    "        \n",
    "        explanations = [list(exp) for exp in row[3]]\n",
    "        \n",
    "        # Combine rationales from annotators\n",
    "        if method == 'union':\n",
    "            # Token is rationale if ANY annotator marked it\n",
    "            final_explanation = [int(any(tokens)) for tokens in zip(*explanations)]\n",
    "        elif method == 'intersection':\n",
    "            # Token is rationale if ALL annotators marked it\n",
    "            final_explanation = [int(all(tokens)) for tokens in zip(*explanations)]\n",
    "        elif method == 'majority':\n",
    "            # Token is rationale if majority of annotators marked it\n",
    "            final_explanation = [int(sum(tokens) >= 2) for tokens in zip(*explanations)]\n",
    "        else:\n",
    "            final_explanation = [int(any(tokens)) for tokens in zip(*explanations)]\n",
    "        \n",
    "        # Create ERASER format document\n",
    "        doc = {\n",
    "            'annotation_id': post_id,\n",
    "            'classification': post_class,\n",
    "            'evidences': [get_evidence(post_id, list(anno_text_list), final_explanation)],\n",
    "            'query': \"What is the class?\",\n",
    "            'query_type': None\n",
    "        }\n",
    "        final_output.append(doc)\n",
    "        \n",
    "        if save_split:\n",
    "            # Save document text\n",
    "            doc_path = os.path.join(save_path, 'docs', post_id)\n",
    "            with open(doc_path, 'w') as fp:\n",
    "                fp.write(' '.join([str(t) for t in list(anno_text_list)]))\n",
    "            \n",
    "            # Save to appropriate split\n",
    "            if post_id in id_division['train']:\n",
    "                train_fp.write(json.dumps(doc) + '\\n')\n",
    "            elif post_id in id_division['val']:\n",
    "                val_fp.write(json.dumps(doc) + '\\n')\n",
    "            elif post_id in id_division['test']:\n",
    "                test_fp.write(json.dumps(doc) + '\\n')\n",
    "    \n",
    "    if save_split:\n",
    "        train_fp.close()\n",
    "        val_fp.close()\n",
    "        test_fp.close()\n",
    "        print(f\"Saved ERASER format files to {save_path}\")\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be21bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data splits\n",
    "with open('./Data/post_id_divisions.json') as fp:\n",
    "    id_division = json.load(fp)\n",
    "\n",
    "print(f\"Train: {len(id_division['train'])} samples\")\n",
    "print(f\"Val:   {len(id_division['val'])} samples\")\n",
    "print(f\"Test:  {len(id_division['test'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4790b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ERASER format\n",
    "method = 'union'  # Options: 'union', 'intersection', 'majority'\n",
    "save_split = True\n",
    "save_path = './Data/Evaluation/Model_Eval/'\n",
    "\n",
    "output_eraser = convert_to_eraser_format(\n",
    "    training_data, \n",
    "    method, \n",
    "    save_split, \n",
    "    save_path, \n",
    "    id_division\n",
    ")\n",
    "print(f\"\\nConverted {len(output_eraser)} samples to ERASER format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify generated files\n",
    "print(\"\\nGenerated files:\")\n",
    "for item in os.listdir(save_path):\n",
    "    item_path = os.path.join(save_path, item)\n",
    "    if os.path.isfile(item_path):\n",
    "        with open(item_path) as f:\n",
    "            line_count = sum(1 for _ in f)\n",
    "        print(f\"  {item}: {line_count} samples\")\n",
    "    else:\n",
    "        doc_count = len(os.listdir(item_path))\n",
    "        print(f\"  {item}/: {doc_count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb62a29",
   "metadata": {},
   "source": [
    "## 5. Run ERASER Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model explanation file mapping\n",
    "explanation_file_mapping = {\n",
    "    'BiRNN-Scrat': 'bestModel_birnnscrat_100_explanation_top5.json',\n",
    "    'BiRNN-Attn': 'bestModel_birnnatt_100_explanation_top5.json',\n",
    "    'CNN-GRU': 'bestModel_cnn_gru_100_explanation_top5.json',\n",
    "    'BERT-Base': 'bestModel_bert_base_uncased_Attn_train_FALSE_100_explanation_top5.json',\n",
    "    'BERT-HateXplain': 'bestModel_bert_base_uncased_Attn_train_TRUE_100_explanation_top5.json',\n",
    "}\n",
    "\n",
    "parent_path = './explanations_dicts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which explanation files exist\n",
    "print(\"Available explanation files:\")\n",
    "available_models = {}\n",
    "\n",
    "for model_name, filename in explanation_file_mapping.items():\n",
    "    filepath = os.path.join(parent_path, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"  ✓ {model_name}: {filename}\")\n",
    "        available_models[model_name] = filename\n",
    "    else:\n",
    "        print(f\"  ✗ {model_name}: {filename} (not found)\")\n",
    "\n",
    "if not available_models:\n",
    "    print(\"\\nNo explanation files found. Run testing_with_rational.py first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6bf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eraser_metrics(model_name, explanation_file, data_dir, output_file):\n",
    "    \"\"\"\n",
    "    Run ERASER benchmark metrics for a model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        explanation_file: Path to model's explanation file\n",
    "        data_dir: Path to ERASER format data directory\n",
    "        output_file: Path to save results\n",
    "    \n",
    "    Returns:\n",
    "        Dict with metric results or None if failed\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    # Build command\n",
    "    cmd = (\n",
    "        f\"cd eraserbenchmark && \"\n",
    "        f\"PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py \"\n",
    "        f\"--split test \"\n",
    "        f\"--data_dir {data_dir} \"\n",
    "        f\"--results {explanation_file} \"\n",
    "        f\"--score_file {output_file}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRunning ERASER metrics for {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return None\n",
    "        \n",
    "        # Load results\n",
    "        with open(output_file) as fp:\n",
    "            return json.load(fp)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e62e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run metrics for available models\n",
    "all_results = {}\n",
    "\n",
    "for model_name, filename in available_models.items():\n",
    "    explanation_file = f\"../{parent_path}{filename}\"\n",
    "    data_dir = \"../Data/Evaluation/Model_Eval\"\n",
    "    output_file = f\"../explainability_results_{model_name.replace('-', '_').lower()}.json\"\n",
    "    \n",
    "    results = run_eraser_metrics(model_name, explanation_file, data_dir, output_file)\n",
    "    if results:\n",
    "        all_results[model_name] = results\n",
    "        print(f\"  ✓ Completed {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcdc828",
   "metadata": {},
   "source": [
    "## 6. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results, model_name):\n",
    "    \"\"\"\n",
    "    Display ERASER benchmark results.\n",
    "    \n",
    "    Args:\n",
    "        results: Dict with metric results\n",
    "        model_name: Name of the model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Plausibility metrics\n",
    "    print(\"\\nPLAUSIBILITY (Agreement with human rationales):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'iou_scores' in results:\n",
    "        iou_f1 = results['iou_scores'][0]['macro']['f1']\n",
    "        print(f\"  IOU F1:        {iou_f1:.4f}\")\n",
    "    \n",
    "    if 'token_prf' in results:\n",
    "        token_f1 = results['token_prf']['instance_macro']['f1']\n",
    "        token_precision = results['token_prf']['instance_macro']['p']\n",
    "        token_recall = results['token_prf']['instance_macro']['r']\n",
    "        print(f\"  Token F1:      {token_f1:.4f}\")\n",
    "        print(f\"  Token Prec:    {token_precision:.4f}\")\n",
    "        print(f\"  Token Recall:  {token_recall:.4f}\")\n",
    "    \n",
    "    if 'token_soft_metrics' in results:\n",
    "        auprc = results['token_soft_metrics']['auprc']\n",
    "        print(f\"  AUPRC:         {auprc:.4f}\")\n",
    "    \n",
    "    # Faithfulness metrics\n",
    "    print(\"\\nFAITHFULNESS (Model reliance on rationales):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'classification_scores' in results:\n",
    "        comp = results['classification_scores'].get('comprehensiveness', 'N/A')\n",
    "        suff = results['classification_scores'].get('sufficiency', 'N/A')\n",
    "        \n",
    "        if isinstance(comp, (int, float)):\n",
    "            print(f\"  Comprehensiveness: {comp:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Comprehensiveness: {comp}\")\n",
    "        \n",
    "        if isinstance(suff, (int, float)):\n",
    "            print(f\"  Sufficiency:       {suff:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Sufficiency:       {suff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for all models\n",
    "for model_name, results in all_results.items():\n",
    "    display_results(results, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35674c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no results were generated, try to load from existing files\n",
    "if not all_results:\n",
    "    print(\"Checking for existing result files...\")\n",
    "    \n",
    "    result_files = [\n",
    "        './model_explain_output.json',\n",
    "        './explainability_results_birnn_scrat.json',\n",
    "    ]\n",
    "    \n",
    "    for filepath in result_files:\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"\\nFound: {filepath}\")\n",
    "            with open(filepath) as fp:\n",
    "                results = json.load(fp)\n",
    "            display_results(results, os.path.basename(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297be8a",
   "metadata": {},
   "source": [
    "## 7. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c16ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "import pandas as pd\n",
    "\n",
    "if all_results:\n",
    "    summary_data = []\n",
    "    \n",
    "    for model_name, results in all_results.items():\n",
    "        row = {'Model': model_name}\n",
    "        \n",
    "        # Plausibility\n",
    "        if 'iou_scores' in results:\n",
    "            row['IOU F1'] = results['iou_scores'][0]['macro']['f1']\n",
    "        if 'token_prf' in results:\n",
    "            row['Token F1'] = results['token_prf']['instance_macro']['f1']\n",
    "        if 'token_soft_metrics' in results:\n",
    "            row['AUPRC'] = results['token_soft_metrics']['auprc']\n",
    "        \n",
    "        # Faithfulness\n",
    "        if 'classification_scores' in results:\n",
    "            row['Comprehensiveness'] = results['classification_scores'].get('comprehensiveness', np.nan)\n",
    "            row['Sufficiency'] = results['classification_scores'].get('sufficiency', np.nan)\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.set_index('Model')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPLAINABILITY METRICS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary_df.round(4).to_string())\n",
    "else:\n",
    "    print(\"No results available. Run testing_with_rational.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b8c37",
   "metadata": {},
   "source": [
    "## 8. Interpretation Guide\n",
    "\n",
    "### Plausibility Metrics (Agreement with human rationales):\n",
    "\n",
    "- **IOU F1**: Intersection over Union F1 score between predicted and ground truth rationale spans.\n",
    "  - Higher is better (closer to 1.0)\n",
    "  - Measures span-level agreement\n",
    "\n",
    "- **Token F1**: Token-level F1 score for rationale identification.\n",
    "  - Higher is better\n",
    "  - Measures agreement at individual token level\n",
    "\n",
    "- **AUPRC**: Area Under Precision-Recall Curve for soft rationale scores.\n",
    "  - Higher is better\n",
    "  - Measures ranking quality of attention scores\n",
    "\n",
    "### Faithfulness Metrics (Model reliance on rationales):\n",
    "\n",
    "- **Comprehensiveness**: How much prediction changes when rationales are removed.\n",
    "  - Higher is better (rationales are necessary for prediction)\n",
    "  - Range: 0 to 1\n",
    "\n",
    "- **Sufficiency**: How well rationales alone predict the label.\n",
    "  - Lower is better (rationales are sufficient for prediction)\n",
    "  - Range: 0 to 1\n",
    "\n",
    "### References:\n",
    "- DeYoung et al. (2020) - \"ERASER: A Benchmark to Evaluate Rationalized NLP Models\"\n",
    "- Mathew et al. (2021) - \"HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
