{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Import data utilities\n",
    "from Preprocess.dataCollect import get_annotated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa9fde",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813be7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration for 2-class (toxic vs non-toxic)\n",
    "dict_data_folder = {\n",
    "    '2': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes_two.npy'},\n",
    "    '3': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes.npy'}\n",
    "}\n",
    "\n",
    "params = {}\n",
    "params['num_classes'] = 2  # toxic vs non-toxic for bias evaluation\n",
    "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "# Load dataset\n",
    "data_all_labelled = get_annotated_data(params)\n",
    "print(f\"Loaded {len(data_all_labelled)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce936d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataset\n",
    "data_all_labelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a41a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(data_all_labelled['final_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b4792",
   "metadata": {},
   "source": [
    "## 2. Extract Target Community Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_information(dataset):\n",
    "    \"\"\"\n",
    "    Extract target community information using majority voting.\n",
    "    A community is selected if at least 2 out of 3 annotators identified it.\n",
    "    \n",
    "    Args:\n",
    "        dataset: DataFrame with annotated data\n",
    "    \n",
    "    Returns:\n",
    "        final_target_output: Dict mapping post_id to target communities\n",
    "        all_communities_selected: List of all selected communities\n",
    "    \"\"\"\n",
    "    final_target_output = defaultdict(list)\n",
    "    all_communities_selected = []\n",
    "    \n",
    "    for each in dataset.iterrows():\n",
    "        # Combine targets from all 3 annotators\n",
    "        all_targets = each[1]['target1'] + each[1]['target2'] + each[1]['target3']\n",
    "        community_dict = dict(Counter(all_targets))\n",
    "        \n",
    "        # Select communities with majority agreement (≥2 annotators)\n",
    "        for key in community_dict:\n",
    "            if community_dict[key] > 1:\n",
    "                final_target_output[each[1]['post_id']].append(key)\n",
    "                all_communities_selected.append(key)\n",
    "        \n",
    "        # Mark as 'None' if no majority consensus\n",
    "        if each[1]['post_id'] not in final_target_output:\n",
    "            final_target_output[each[1]['post_id']].append('None')\n",
    "            all_communities_selected.append('None')\n",
    "\n",
    "    return final_target_output, all_communities_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate target information\n",
    "target_information, all_communities_selected = generate_target_information(data_all_labelled)\n",
    "print(f\"Extracted target info for {len(target_information)} posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community statistics\n",
    "community_count_dict = Counter(all_communities_selected)\n",
    "print(\"\\nAll communities (sorted by frequency):\")\n",
    "for community, count in community_count_dict.most_common():\n",
    "    print(f\"  {community}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 10 communities (excluding 'None' and 'Other')\n",
    "community_count_filtered = community_count_dict.copy()\n",
    "community_count_filtered.pop('None', None)\n",
    "community_count_filtered.pop('Other', None)\n",
    "\n",
    "list_selected_community = [community for community, _ in community_count_filtered.most_common(10)]\n",
    "print(f\"\\nSelected top 10 communities for bias evaluation:\")\n",
    "for i, community in enumerate(list_selected_community, 1):\n",
    "    print(f\"  {i}. {community} ({community_count_dict[community]} mentions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfde85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter target information to include only top 10 communities\n",
    "final_target_information = {}\n",
    "for post_id in target_information:\n",
    "    matched_communities = list(set(target_information[post_id]) & set(list_selected_community))\n",
    "    final_target_information[post_id] = matched_communities if matched_communities else None\n",
    "\n",
    "# Add to dataset\n",
    "data_all_labelled['final_target_category'] = data_all_labelled['post_id'].map(final_target_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a31fb",
   "metadata": {},
   "source": [
    "## 3. Filter to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test split IDs\n",
    "with open('./Data/post_id_divisions.json', 'r') as fp:\n",
    "    post_id_dict = json.load(fp)\n",
    "\n",
    "print(f\"Train: {len(post_id_dict['train'])} samples\")\n",
    "print(f\"Val:   {len(post_id_dict['val'])} samples\")\n",
    "print(f\"Test:  {len(post_id_dict['test'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abb2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to test set\n",
    "data_all_labelled_bias = data_all_labelled[data_all_labelled['post_id'].isin(post_id_dict['test'])].copy()\n",
    "print(f\"\\nFiltered to {len(data_all_labelled_bias)} test samples for bias evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ba350",
   "metadata": {},
   "source": [
    "## 4. Define Bias Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7bec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_score(label_name, label_dict):\n",
    "    \"\"\"\n",
    "    Convert classification label to toxicity score [0, 1].\n",
    "    Higher score = more toxic.\n",
    "    \n",
    "    Args:\n",
    "        label_name: Predicted label ('toxic' or 'non-toxic')\n",
    "        label_dict: Dict with prediction probabilities\n",
    "    \n",
    "    Returns:\n",
    "        Toxicity score between 0 and 1\n",
    "    \"\"\"\n",
    "    if label_name == 'non-toxic':\n",
    "        return 1 - label_dict[label_name]\n",
    "    else:\n",
    "        return label_dict[label_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c02cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_evaluation_metric(dataset, method, community):\n",
    "    \"\"\"\n",
    "    Divide post IDs into positive (toxic) and negative (non-toxic) sets\n",
    "    based on the bias evaluation method.\n",
    "    \n",
    "    Methods:\n",
    "        - subgroup: Posts mentioning the community\n",
    "        - bpsn: Background Positive, Subgroup Negative\n",
    "        - bnsp: Background Negative, Subgroup Positive\n",
    "    \n",
    "    Args:\n",
    "        dataset: DataFrame with test data\n",
    "        method: One of 'subgroup', 'bpsn', 'bnsp'\n",
    "        community: Target community name\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'positiveID' and 'negativeID' lists\n",
    "    \"\"\"\n",
    "    positive_ids = []\n",
    "    negative_ids = []\n",
    "    \n",
    "    for _, row in dataset.iterrows():\n",
    "        if row['final_target_category'] is None:\n",
    "            continue\n",
    "        \n",
    "        is_community = community in row['final_target_category']\n",
    "        is_toxic = row['final_label'] != 'non-toxic'\n",
    "        \n",
    "        if method == 'subgroup':\n",
    "            # Subgroup AUC: only posts mentioning the community\n",
    "            if is_community:\n",
    "                if is_toxic:\n",
    "                    positive_ids.append(row['post_id'])\n",
    "                else:\n",
    "                    negative_ids.append(row['post_id'])\n",
    "        \n",
    "        elif method == 'bpsn':\n",
    "            # BPSN AUC: Measures false positive bias\n",
    "            # Positive: toxic posts NOT mentioning community\n",
    "            # Negative: non-toxic posts mentioning community\n",
    "            if is_community and not is_toxic:\n",
    "                negative_ids.append(row['post_id'])\n",
    "            elif not is_community and is_toxic:\n",
    "                positive_ids.append(row['post_id'])\n",
    "        \n",
    "        elif method == 'bnsp':\n",
    "            # BNSP AUC: Measures false negative bias\n",
    "            # Positive: toxic posts mentioning community\n",
    "            # Negative: non-toxic posts NOT mentioning community\n",
    "            if is_community and is_toxic:\n",
    "                positive_ids.append(row['post_id'])\n",
    "            elif not is_community and not is_toxic:\n",
    "                negative_ids.append(row['post_id'])\n",
    "        \n",
    "        else:\n",
    "            print(f'Unknown method: {method}')\n",
    "    \n",
    "    return {'positiveID': positive_ids, 'negativeID': negative_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d82bb",
   "metadata": {},
   "source": [
    "## 5. Calculate Bias Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Configuration\n",
    "parent_path = './explanations_dicts/'\n",
    "method_list = ['subgroup', 'bpsn', 'bnsp']\n",
    "community_list = list(list_selected_community)\n",
    "\n",
    "# Model bias score file mapping\n",
    "# Add your model files here\n",
    "bias_score_file_mapping = {\n",
    "    'BiRNN-Attn': 'bestModel_birnnatt_bias.json',\n",
    "    'BiRNN-Scrat': 'bestModel_birnnscrat_bias.json',\n",
    "    'CNN-GRU': 'bestModel_cnn_gru_bias.json',\n",
    "    'BERT-Base': 'bestModel_bert_base_uncased_Attn_train_FALSE_bias.json',\n",
    "    'BERT-HateXplain': 'bestModel_bert_base_uncased_Attn_train_TRUE_bias.json',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which bias files exist\n",
    "print(\"Available bias score files:\")\n",
    "available_models = {}\n",
    "for model_name, filename in bias_score_file_mapping.items():\n",
    "    filepath = os.path.join(parent_path, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"  ✓ {model_name}: {filename}\")\n",
    "        available_models[model_name] = filename\n",
    "    else:\n",
    "        print(f\"  ✗ {model_name}: {filename} (not found)\")\n",
    "\n",
    "if not available_models:\n",
    "    print(\"\\nNo bias score files found. Run testing_for_bias.py first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3fbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bias scores for each model\n",
    "final_bias_dictionary = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for model_name in tqdm(available_models, desc=\"Processing models\"):\n",
    "    filepath = os.path.join(parent_path, available_models[model_name])\n",
    "    \n",
    "    # Load model predictions\n",
    "    total_data = {}\n",
    "    with open(filepath) as fp:\n",
    "        for line in fp:\n",
    "            data = json.loads(line)\n",
    "            total_data[data['annotation_id']] = data\n",
    "    \n",
    "    # Calculate AUC for each method and community\n",
    "    for method in method_list:\n",
    "        for community in community_list:\n",
    "            community_data = bias_evaluation_metric(data_all_labelled_bias, method, community)\n",
    "            \n",
    "            truth_values = []\n",
    "            prediction_values = []\n",
    "            label_to_value = {'toxic': 1.0, 'non-toxic': 0.0}\n",
    "            \n",
    "            # Collect positive samples\n",
    "            for post_id in community_data['positiveID']:\n",
    "                if post_id in total_data:\n",
    "                    truth_values.append(label_to_value[total_data[post_id]['ground_truth']])\n",
    "                    prediction_values.append(convert_to_score(\n",
    "                        total_data[post_id]['classification'],\n",
    "                        total_data[post_id]['classification_scores']\n",
    "                    ))\n",
    "            \n",
    "            # Collect negative samples\n",
    "            for post_id in community_data['negativeID']:\n",
    "                if post_id in total_data:\n",
    "                    truth_values.append(label_to_value[total_data[post_id]['ground_truth']])\n",
    "                    prediction_values.append(convert_to_score(\n",
    "                        total_data[post_id]['classification'],\n",
    "                        total_data[post_id]['classification_scores']\n",
    "                    ))\n",
    "            \n",
    "            # Calculate AUC if both classes present\n",
    "            if len(truth_values) > 0 and len(set(truth_values)) > 1:\n",
    "                auc_score = roc_auc_score(truth_values, prediction_values)\n",
    "                final_bias_dictionary[model_name][method][community] = auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83ede1",
   "metadata": {},
   "source": [
    "## 6. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ed53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display per-community bias scores\n",
    "import pandas as pd\n",
    "\n",
    "for model_name in final_bias_dictionary:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for method in method_list:\n",
    "        print(f\"\\n{method.upper()} AUC:\")\n",
    "        for community in final_bias_dictionary[model_name][method]:\n",
    "            score = final_bias_dictionary[model_name][method][community]\n",
    "            print(f\"  {community:20s}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate generalized mean of bias scores\n",
    "# Using power value of -5 as in the original paper\n",
    "power_value = -5\n",
    "num_communities = len(community_list)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERALIZED MEAN BIAS SCORES (p=-5)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':25s} {'Subgroup':>12s} {'BPSN':>12s} {'BNSP':>12s}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for model_name in final_bias_dictionary:\n",
    "    scores = []\n",
    "    for method in method_list:\n",
    "        temp_values = []\n",
    "        for community in final_bias_dictionary[model_name][method]:\n",
    "            temp_values.append(\n",
    "                pow(final_bias_dictionary[model_name][method][community], power_value)\n",
    "            )\n",
    "        if temp_values:\n",
    "            gen_mean = pow(np.sum(temp_values) / num_communities, 1 / power_value)\n",
    "            scores.append(gen_mean)\n",
    "        else:\n",
    "            scores.append(0.0)\n",
    "    \n",
    "    if scores:\n",
    "        print(f\"{model_name:25s} {scores[0]:12.4f} {scores[1]:12.4f} {scores[2]:12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "output_file = './bias_evaluation_results.json'\n",
    "\n",
    "# Convert defaultdict to regular dict for JSON serialization\n",
    "results_dict = {\n",
    "    model: {\n",
    "        method: dict(communities)\n",
    "        for method, communities in methods.items()\n",
    "    }\n",
    "    for model, methods in final_bias_dictionary.items()\n",
    "}\n",
    "\n",
    "with open(output_file, 'w') as fp:\n",
    "    json.dump(results_dict, fp, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cbeeea",
   "metadata": {},
   "source": [
    "## 7. Interpretation Guide\n",
    "\n",
    "### Bias Metrics:\n",
    "\n",
    "- **Subgroup AUC**: Measures model performance on posts mentioning a specific community. \n",
    "  - Higher is better (closer to 1.0)\n",
    "  - Low score indicates poor discrimination for that subgroup\n",
    "\n",
    "- **BPSN (Background Positive, Subgroup Negative) AUC**: Measures false positive bias.\n",
    "  - Higher is better\n",
    "  - Low score indicates the model may incorrectly classify benign mentions of a community as toxic\n",
    "\n",
    "- **BNSP (Background Negative, Subgroup Positive) AUC**: Measures false negative bias.\n",
    "  - Higher is better\n",
    "  - Low score indicates the model may miss toxic content targeting a community\n",
    "\n",
    "### Generalized Mean:\n",
    "The generalized mean with p=-5 emphasizes lower scores, making it sensitive to worst-case performance across communities.\n",
    "\n",
    "### References:\n",
    "- Borkan et al. (2019) - \"Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
