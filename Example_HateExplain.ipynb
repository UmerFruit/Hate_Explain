{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7EGDu9Wbvnm2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EGDu9Wbvnm2",
    "outputId": "fd45dd20-1778-4f56-c788-d41fb3e8900a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# GOOGLE COLAB SETUP - Mount Drive & Extract Data\n",
    "# ================================================\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1595d1ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1595d1ba",
    "outputId": "78c5a680-30b6-4006-da0b-284e89cd4bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Hate_Explain'...\n",
      "remote: Enumerating objects: 103, done.\u001b[K\n",
      "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
      "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
      "remote: Total 103 (delta 32), reused 99 (delta 28), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (103/103), 2.35 MiB | 4.79 MiB/s, done.\n",
      "Resolving deltas: 100% (32/32), done.\n",
      "/content/Hate_Explain\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/UmerFruit/Hate_Explain.git\n",
    "%cd Hate_Explain\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a557a518",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a557a518",
    "outputId": "9e55803d-c32d-4eea-8ae9-1c8c8afeca52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting copy\n",
      "Starting extraction\n",
      "Done extraction\n",
      "\n",
      "GPU Available: True\n",
      "GPU Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE SMALL DATASET (GIVES KEY ERRORS SINCE IT DOESNT HAVE THE MOST OF THE WORDS)\n",
    "# !cp /content/drive/MyDrive/glove.42B.300d.small.zip ./Data/\n",
    "# Extract the zip file\n",
    "# !unzip -q ./Data/glove.42B.300d.small.zip -d ./Data/\n",
    "# !mv ./Data/glove.42B.300d.small.txt  ./Data/glove.42B.300d.txt\n",
    "\n",
    "# LOAD THE FULL DATASET\n",
    "print(\"Starting copy\")\n",
    "!cp /content/drive/MyDrive/glove.42B.300d.zip ./Data/\n",
    "# Extract the zip file\n",
    "print(\"Starting extraction\")\n",
    "!unzip -q ./Data/glove.42B.300d.zip -d ./Data/\n",
    "print(\"Done extraction\")\n",
    "\n",
    "# Clean up zip file (if needed)\n",
    "# !rm ./Data/glove.42B.300d.small.zip\n",
    "# !rm ./Data/glove.42B.300d.zip\n",
    "\n",
    "# Copy the model files\n",
    "!cp /content/drive/MyDrive/word2vec.model /content/Hate_Explain/Data\n",
    "!cp /content/drive/MyDrive/word2vec.model.vectors.npy /content/Hate_Explain/Data\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"\\nGPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64eb58",
   "metadata": {
    "id": "7b64eb58"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc721be3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c630f3f1",
   "metadata": {
    "id": "c630f3f1"
   },
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed11d7bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed11d7bc",
    "outputId": "8955d2f0-ecc1-4f12-d8d8-36ecb5fafcc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create necessary directories\n",
    "import os\n",
    "os.makedirs('Saved', exist_ok=True)\n",
    "os.makedirs('explanations_dicts', exist_ok=True)\n",
    "print(\"Directories created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e725d768",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e725d768",
    "outputId": "217e8173-9fbe-40be-89a1-62de33fff415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.9.0+cu126)\n",
      "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.57.2)\n",
      "Requirement already satisfied: scipy>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (1.16.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.0.2)\n",
      "Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
      "Requirement already satisfied: spacy>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.8.11)\n",
      "Requirement already satisfied: gensim>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (4.4.0)\n",
      "Requirement already satisfied: ekphrasis>=0.5.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.5.4)\n",
      "Requirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (3.10.0)\n",
      "Requirement already satisfied: tqdm>=4.66.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (4.67.1)\n",
      "Requirement already satisfied: lime>=0.2.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 19)) (0.2.0.1)\n",
      "Requirement already satisfied: GPUtil>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (1.4.0)\n",
      "Requirement already satisfied: more-itertools>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 21)) (10.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.0->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.0->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.0->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 7)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (0.20.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (2.12.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim>=4.3.0->-r requirements.txt (line 11)) (7.5.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from ekphrasis>=0.5.4->-r requirements.txt (line 12)) (3.2.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from ekphrasis>=0.5.4->-r requirements.txt (line 12)) (0.4.6)\n",
      "Requirement already satisfied: ujson in /usr/local/lib/python3.12/dist-packages (from ekphrasis>=0.5.4->-r requirements.txt (line 12)) (5.11.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from ekphrasis>=0.5.4->-r requirements.txt (line 12)) (3.9.1)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from ekphrasis>=0.5.4->-r requirements.txt (line 12)) (6.3.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (3.2.5)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime>=0.2.0.1->-r requirements.txt (line 19)) (0.25.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.7.0->-r requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.7.0->-r requirements.txt (line 10)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.7.0->-r requirements.txt (line 10)) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.36.0->-r requirements.txt (line 3)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.36.0->-r requirements.txt (line 3)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.36.0->-r requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.36.0->-r requirements.txt (line 3)) (2025.11.12)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime>=0.2.0.1->-r requirements.txt (line 19)) (2.37.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime>=0.2.0.1->-r requirements.txt (line 19)) (2025.10.16)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime>=0.2.0.1->-r requirements.txt (line 19)) (0.4)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim>=4.3.0->-r requirements.txt (line 11)) (2.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.7.0->-r requirements.txt (line 10)) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.7.0->-r requirements.txt (line 10)) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy>=3.7.0->-r requirements.txt (line 10)) (8.3.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy>=3.7.0->-r requirements.txt (line 10)) (0.23.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->ekphrasis>=0.5.4->-r requirements.txt (line 12)) (0.2.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->-r requirements.txt (line 2)) (3.0.3)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# Install required packages (run this if not already installed)\n",
    "!pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f2403",
   "metadata": {
    "id": "931f2403"
   },
   "source": [
    "## 2. Download and Prepare GloVe Embeddings\n",
    "\n",
    "**Note:** This step is only required once. Skip if you already have the file. Like i did with in the google drive mounted. If not it downloads it right here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c64cf",
   "metadata": {
    "id": "ca9c64cf"
   },
   "outputs": [],
   "source": [
    "# Download GloVe embeddings (only run if needed)\n",
    "# !wget http://nlp.stanford.edu/data/glove.42B.300d.zip -P Data/\n",
    "# !unzip Data/glove.42B.300d.zip -d Data/\n",
    "# !rm Data/glove.42B.300d.zip\n",
    "# print(\"GloVe embeddings downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "044d03bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "044d03bc",
    "outputId": "d42c4a2d-ed87-4e75-af80-74f5c04fe8c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting GloVe to Word2Vec format...\n",
      "Loading and saving model (this may take a few minutes)...\n",
      "Done! word2vec.model saved.\n"
     ]
    }
   ],
   "source": [
    "# Convert GloVe to Word2Vec format (Run if model values are not already saved)\n",
    "# from gensim.models import KeyedVectors\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# # Convert GloVe format to Word2Vec format\n",
    "# print(\"Converting GloVe to Word2Vec format...\")\n",
    "# glove2word2vec('Data/glove.42B.300d.txt', 'Data/glove.42B.300d_w2v.txt')\n",
    "\n",
    "# # Load and save in gensim format\n",
    "# print(\"Loading and saving model (this may take a few minutes)...\")\n",
    "# word2vecmodel1 = KeyedVectors.load_word2vec_format('Data/glove.42B.300d_w2v.txt', binary=False)\n",
    "# word2vecmodel1.save(\"Data/word2vec.model\")\n",
    "\n",
    "# # Clean up intermediate files\n",
    "# import gc\n",
    "# del word2vecmodel1\n",
    "# gc.collect()\n",
    "\n",
    "# # Remove large text files to save space\n",
    "# import os\n",
    "# os.remove('Data/glove.42B.300d.txt')\n",
    "# os.remove('Data/glove.42B.300d_w2v.txt')\n",
    "# print(\"Done! word2vec.model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12878bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/Hate_Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cmUEkptetfxW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmUEkptetfxW",
    "outputId": "e61a298f-fecb-46d7-8179-3ae2ffa2d9c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Removing existing directory: Data/Total_data_normal_softmax_1_128_3\n",
      "‚úì Directory removed, will regenerate fresh files...\n",
      "üóëÔ∏è  Removing existing pickle file: Data/Total_data_normal_softmax_1_128_3.pickle\n",
      "‚úì Pickle file removed\n",
      "\n",
      "üìä Generating vocabulary and embeddings...\n",
      "   Expected directory: Data/Total_data_normal_softmax_1_128_3\n",
      "   Expected pickle: Data/Total_data_normal_softmax_1_128_3.pickle\n",
      "total_data 20148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20148/20148 [00:57<00:00, 352.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_error: 0\n",
      "no_majority: 919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 2597/15383 [00:00<00:00, 13095.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15383/15383 [00:01<00:00, 13574.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22236, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15383/15383 [00:00<00:00, 22106.83it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1922/1922 [00:00<00:00, 21508.62it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1924/1924 [00:00<00:00, 21665.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total dataset size: 19229\n",
      "\n",
      "‚úì Vocabulary files generated successfully!\n",
      "   Training samples: 15383\n",
      "   Validation samples: 1922\n",
      "   Test samples: 1924\n",
      "\n",
      "‚úì Vocab pickle file created: 2,417,514,432 bytes (2305.5 MB)\n",
      "  Location: Data/Total_data_normal_softmax_1_128_3/vocab_own.pickle\n",
      "‚úì Pickle file is valid and loadable!\n",
      "  Vocabulary size: 22236\n",
      "  Embedding shape: (22236, 300)\n"
     ]
    }
   ],
   "source": [
    "# Generate vocabulary embeddings\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/content/Hate_Explain')\n",
    "\n",
    "from TensorDataset.datsetSplitter import createDatasetSplit\n",
    "\n",
    "# IMPORTANT: These parameters must match training configuration!\n",
    "# Especially: variance, type_attention, max_length, num_classes\n",
    "params = {\n",
    "    # Data parameters\n",
    "    'num_classes': 3,\n",
    "    'data_file': 'Data/dataset.json',\n",
    "    'class_names': 'Data/classes.npy',\n",
    "\n",
    "    # Tokenization parameters\n",
    "    'bert_tokens': False,\n",
    "    'max_length': 128,\n",
    "    'include_special': False,\n",
    "\n",
    "    # Attention parameters - MUST MATCH TRAINING\n",
    "    'type_attention': 'softmax',\n",
    "    'variance': 1,  # ‚Üê Changed to match training (was 5)\n",
    "    'decay': False,\n",
    "    'method': 'additive',\n",
    "    'window': 4,\n",
    "    'alpha': 0.5,\n",
    "    'p_value': 0.8,\n",
    "\n",
    "    # Label parameters\n",
    "    'majority': 2,\n",
    "\n",
    "    # Preprocessing control\n",
    "    'not_recollect': True,  # Not used by createDatasetSplit\n",
    "\n",
    "    # Other required parameters\n",
    "    'random_seed': 42,\n",
    "    'normalized': False\n",
    "}\n",
    "\n",
    "# FORCE REGENERATION: Delete existing directory if it exists\n",
    "from Preprocess.dataCollect import set_name\n",
    "filename = set_name(params)\n",
    "vocab_dir = filename[:-7]  # Remove '.pickle' extension to get directory name\n",
    "\n",
    "if os.path.exists(vocab_dir):\n",
    "    import shutil\n",
    "    print(f\"üóëÔ∏è  Removing existing directory: {vocab_dir}\")\n",
    "    shutil.rmtree(vocab_dir)\n",
    "    print(\"‚úì Directory removed, will regenerate fresh files...\")\n",
    "\n",
    "# Also remove the pickle file if it exists\n",
    "if os.path.exists(filename):\n",
    "    print(f\"üóëÔ∏è  Removing existing pickle file: {filename}\")\n",
    "    os.remove(filename)\n",
    "    print(\"‚úì Pickle file removed\")\n",
    "\n",
    "print(\"\\nüìä Generating vocabulary and embeddings...\")\n",
    "print(f\"   Expected directory: {vocab_dir}\")\n",
    "print(f\"   Expected pickle: {filename}\")\n",
    "\n",
    "# This function will:\n",
    "# 1. Call collect_data() to create the main pickle file\n",
    "# 2. Create train/val/test splits\n",
    "# 3. Build vocabulary from word2vec embeddings\n",
    "# 4. Create directory with vocab_own.pickle and split files\n",
    "train, val, test, vocab_own = createDatasetSplit(params)\n",
    "\n",
    "print(f\"\\n‚úì Vocabulary files generated successfully!\")\n",
    "print(f\"   Training samples: {len(train)}\")\n",
    "print(f\"   Validation samples: {len(val)}\")\n",
    "print(f\"   Test samples: {len(test)}\")\n",
    "\n",
    "# Verify the pickle file was created\n",
    "vocab_pickle_path = f'{vocab_dir}/vocab_own.pickle'\n",
    "\n",
    "if os.path.exists(vocab_pickle_path):\n",
    "    file_size = os.path.getsize(vocab_pickle_path)\n",
    "    print(f\"\\n‚úì Vocab pickle file created: {file_size:,} bytes ({file_size/1024/1024:.1f} MB)\")\n",
    "    print(f\"  Location: {vocab_pickle_path}\")\n",
    "\n",
    "    # Verify it's valid\n",
    "    try:\n",
    "        import pickle\n",
    "        with open(vocab_pickle_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"‚úì Pickle file is valid and loadable!\")\n",
    "        print(f\"  Vocabulary size: {len(data.vocab)}\")\n",
    "        print(f\"  Embedding shape: {data.embeddings.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó ERROR: Pickle file is corrupted: {str(e)}\")\n",
    "else:\n",
    "    print(f\"\\n‚úó ERROR: Vocab pickle file not found at {vocab_pickle_path}\")\n",
    "    if os.path.exists(vocab_dir):\n",
    "        print(f\"  Directory exists, checking contents:\")\n",
    "        for f in os.listdir(vocab_dir):\n",
    "            filepath = os.path.join(vocab_dir, f)\n",
    "            size = os.path.getsize(filepath)\n",
    "            print(f\"    - {f} ({size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"  Directory {vocab_dir} not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938582a",
   "metadata": {
    "id": "7938582a"
   },
   "source": [
    "## 3. Import Dependencies and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff1eed7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ff1eed7",
    "outputId": "161525d4-fe07-40a3-ac23-af14711a6642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "# Import the training module\n",
    "from manual_training_inference import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9079f11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9079f11",
    "outputId": "5656e6f8-3c05-405d-9531-7a3e77da260e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Load model parameters from JSON configuration\n",
    "import json\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "path_file = 'best_model_json/bestModel_birnnscrat.json'\n",
    "with open(path_file, mode='r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Convert string values to appropriate types\n",
    "for key in params:\n",
    "    if params[key] == 'True':\n",
    "        params[key] = True\n",
    "    elif params[key] == 'False':\n",
    "        params[key] = False\n",
    "    if key in ['batch_size', 'num_classes', 'hidden_size', 'supervised_layer_pos',\n",
    "               'num_supervised_heads', 'random_seed', 'max_length']:\n",
    "        if params[key] != 'N/A':\n",
    "            params[key] = int(params[key])\n",
    "    if (key == 'weights') and (params['auto_weights'] == False):\n",
    "        params[key] = ast.literal_eval(params[key])\n",
    "\n",
    "# Configure for Colab execution\n",
    "params['logging'] = 'local'\n",
    "params['device'] = 'cuda'  # Use GPU in Colab\n",
    "params['best_params'] = False\n",
    "\n",
    "# Setup device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('WARNING: GPU not available. Using CPU (training will be slow).')\n",
    "    print('Go to Runtime ‚Üí Change runtime type ‚Üí GPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d7f249",
   "metadata": {
    "id": "01d7f249"
   },
   "outputs": [],
   "source": [
    "# Data folder configuration\n",
    "dict_data_folder = {\n",
    "    '2': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes_two.npy'},\n",
    "    '3': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes.npy'}\n",
    "}\n",
    "\n",
    "# Configure training parameters\n",
    "params['variance'] = 1\n",
    "params['epochs'] = 5  # Reduce for faster testing\n",
    "params['to_save'] = True\n",
    "\n",
    "# Ensure critical parameters are set (in case they're missing)\n",
    "if 'auto_weights' not in params:\n",
    "    params['auto_weights'] = True\n",
    "if 'att_lambda' not in params:\n",
    "    params['att_lambda'] = 1.0\n",
    "if 'train_att' not in params:\n",
    "    params['train_att'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d1e4a47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d1e4a47",
    "outputId": "5970eee7-a94f-4331-f9b7-3dd16b468334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2-class model...\n",
      "[1.2301791 0.8423818]\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:19, 24.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 295.31358239060876\n",
      "model previously passed\n",
      "Running eval on  train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:00, 516.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.64\n",
      " Fscore: 0.63\n",
      " Precision: 0.73\n",
      " Recall: 0.69\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:01\n",
      "model previously passed\n",
      "Running eval on  val ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 458.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.62\n",
      " Fscore: 0.61\n",
      " Precision: 0.72\n",
      " Recall: 0.67\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:00\n",
      "model previously passed\n",
      "Running eval on  test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 568.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.62\n",
      " Fscore: 0.61\n",
      " Precision: 0.71\n",
      " Recall: 0.67\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:00\n",
      "  Test - fscore: 0.6108, accuracy: 0.6190\n",
      "  Val  - fscore: 0.6090, accuracy: 0.6186\n",
      "  Train- fscore: 0.6281, accuracy: 0.6355\n",
      "0.6090377004692404 0\n",
      "Saving model\n",
      "Saved/birnnscrat_lstm_64_2_100.pth\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:11, 41.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 295.190023949637\n",
      "model previously passed\n",
      "Running eval on  train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:00, 554.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.71\n",
      " Fscore: 0.71\n",
      " Precision: 0.76\n",
      " Recall: 0.75\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:01\n",
      "model previously passed\n",
      "Running eval on  val ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 492.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.66\n",
      " Fscore: 0.66\n",
      " Precision: 0.73\n",
      " Recall: 0.70\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:00\n",
      "model previously passed\n",
      "Running eval on  test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 567.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.67\n",
      " Fscore: 0.67\n",
      " Precision: 0.73\n",
      " Recall: 0.71\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:00\n",
      "  Test - fscore: 0.6684, accuracy: 0.6705\n",
      "  Val  - fscore: 0.6606, accuracy: 0.6634\n",
      "  Train- fscore: 0.7125, accuracy: 0.7133\n",
      "0.6606218896307607 0.6090377004692404\n",
      "Saving model\n",
      "Saved/birnnscrat_lstm_64_2_100.pth\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:11, 41.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 295.1041867579343\n",
      "model previously passed\n",
      "Running eval on  train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:00, 549.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.81\n",
      " Fscore: 0.81\n",
      " Precision: 0.81\n",
      " Recall: 0.82\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:01\n",
      "model previously passed\n",
      "Running eval on  val ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 580.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.71\n",
      " Fscore: 0.71\n",
      " Precision: 0.73\n",
      " Recall: 0.73\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:00\n",
      "model previously passed\n",
      "Running eval on  test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 576.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.72\n",
      " Fscore: 0.72\n",
      " Precision: 0.73\n",
      " Recall: 0.74\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:00\n",
      "  Test - fscore: 0.7190, accuracy: 0.7193\n",
      "  Val  - fscore: 0.7131, accuracy: 0.7133\n",
      "  Train- fscore: 0.8085, accuracy: 0.8096\n",
      "0.713133007357349 0.6606218896307607\n",
      "Saving model\n",
      "Saved/birnnscrat_lstm_64_2_100.pth\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:11, 41.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 295.0351021939157\n",
      "model previously passed\n",
      "Running eval on  train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:00, 539.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.86\n",
      " Fscore: 0.86\n",
      " Precision: 0.86\n",
      " Recall: 0.87\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:01\n",
      "model previously passed\n",
      "Running eval on  val ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 574.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.73\n",
      " Fscore: 0.73\n",
      " Precision: 0.73\n",
      " Recall: 0.74\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:00\n",
      "model previously passed\n",
      "Running eval on  test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 504.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.72\n",
      " Fscore: 0.72\n",
      " Precision: 0.72\n",
      " Recall: 0.73\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:00\n",
      "  Test - fscore: 0.7189, accuracy: 0.7214\n",
      "  Val  - fscore: 0.7315, accuracy: 0.7336\n",
      "  Train- fscore: 0.8616, accuracy: 0.8641\n",
      "0.7314602264974758 0.713133007357349\n",
      "Saving model\n",
      "Saved/birnnscrat_lstm_64_2_100.pth\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:11, 41.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 294.9944699420255\n",
      "model previously passed\n",
      "Running eval on  train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:00, 557.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.76\n",
      " Fscore: 0.76\n",
      " Precision: 0.81\n",
      " Recall: 0.80\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:01\n",
      "model previously passed\n",
      "Running eval on  val ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 566.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.63\n",
      " Fscore: 0.63\n",
      " Precision: 0.71\n",
      " Recall: 0.68\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:00\n",
      "model previously passed\n",
      "Running eval on  test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 573.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.63\n",
      " Fscore: 0.63\n",
      " Precision: 0.72\n",
      " Recall: 0.68\n",
      " Roc Auc: 0.00\n",
      " Test took: 0:00:00\n",
      "  Test - fscore: 0.6252, accuracy: 0.6315\n",
      "  Val  - fscore: 0.6258, accuracy: 0.6316\n",
      "  Train- fscore: 0.7612, accuracy: 0.7616\n",
      "best_val_fscore 0.7314602264974758\n",
      "best_test_fscore 0.7188983855650521\n",
      "best_val_rocauc 0\n",
      "best_test_rocauc 0\n",
      "best_val_precision 0.7344066913781584\n",
      "best_test_precision 0.7212201763485477\n",
      "best_val_recall 0.7429125786509352\n",
      "best_test_recall 0.7292451435763523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with 2 classes (toxic vs non-toxic)\n",
    "params['num_classes'] = 2\n",
    "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "if params['num_classes'] == 2 and params['auto_weights'] == False:\n",
    "    params['weights'] = [1.0, 1.0]\n",
    "\n",
    "print(f\"Training {params['num_classes']}-class model...\")\n",
    "train_model(params, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0fc73b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf0fc73b",
    "outputId": "1aa56282-95cd-481c-9bec-8a9b3b88f099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 3-class model...\n",
      "[1.0796857 0.8201194 1.1703163]\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:11, 41.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 295.68468743401604\n",
      "model previously passed\n",
      "Running eval on  train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:00, 537.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.61\n",
      " Fscore: 0.58\n",
      " Precision: 0.63\n",
      " Recall: 0.58\n",
      " Roc Auc: 0.79\n",
      " Test took: 0:00:01\n",
      "model previously passed\n",
      "Running eval on  val ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 573.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.60\n",
      " Fscore: 0.56\n",
      " Precision: 0.63\n",
      " Recall: 0.56\n",
      " Roc Auc: 0.77\n",
      " Test took: 0:00:00\n",
      "model previously passed\n",
      "Running eval on  test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 577.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.59\n",
      " Fscore: 0.55\n",
      " Precision: 0.62\n",
      " Recall: 0.55\n",
      " Roc Auc: 0.77\n",
      " Test took: 0:00:00\n",
      "  Test - fscore: 0.5524, accuracy: 0.5878\n",
      "  Val  - fscore: 0.5617, accuracy: 0.5963\n",
      "  Train- fscore: 0.5782, accuracy: 0.6105\n",
      "0.5616702167867701 0\n",
      "Saving model\n",
      "Saved/birnnscrat_lstm_64_3_100.pth\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:11, 41.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 295.5435634938198\n",
      "model previously passed\n",
      "Running eval on  train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:00, 547.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.69\n",
      " Fscore: 0.67\n",
      " Precision: 0.69\n",
      " Recall: 0.67\n",
      " Roc Auc: 0.84\n",
      " Test took: 0:00:01\n",
      "model previously passed\n",
      "Running eval on  val ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 551.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.62\n",
      " Fscore: 0.60\n",
      " Precision: 0.62\n",
      " Recall: 0.60\n",
      " Roc Auc: 0.79\n",
      " Test took: 0:00:00\n",
      "model previously passed\n",
      "Running eval on  test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 580.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.64\n",
      " Fscore: 0.61\n",
      " Precision: 0.64\n",
      " Recall: 0.61\n",
      " Roc Auc: 0.79\n",
      " Test took: 0:00:00\n",
      "  Test - fscore: 0.6150, accuracy: 0.6362\n",
      "  Val  - fscore: 0.5992, accuracy: 0.6223\n",
      "  Train- fscore: 0.6704, accuracy: 0.6873\n",
      "0.5992262854884399 0.5616702167867701\n",
      "Saving model\n",
      "Saved/birnnscrat_lstm_64_3_100.pth\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:11, 41.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 295.4403857193469\n",
      "model previously passed\n",
      "Running eval on  train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:00, 543.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.74\n",
      " Fscore: 0.73\n",
      " Precision: 0.74\n",
      " Recall: 0.73\n",
      " Roc Auc: 0.89\n",
      " Test took: 0:00:01\n",
      "model previously passed\n",
      "Running eval on  val ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 578.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.65\n",
      " Fscore: 0.63\n",
      " Precision: 0.65\n",
      " Recall: 0.63\n",
      " Roc Auc: 0.81\n",
      " Test took: 0:00:00\n",
      "model previously passed\n",
      "Running eval on  test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 577.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.65\n",
      " Fscore: 0.63\n",
      " Precision: 0.65\n",
      " Recall: 0.63\n",
      " Roc Auc: 0.81\n",
      " Test took: 0:00:00\n",
      "  Test - fscore: 0.6304, accuracy: 0.6492\n",
      "  Val  - fscore: 0.6292, accuracy: 0.6483\n",
      "  Train- fscore: 0.7318, accuracy: 0.7435\n",
      "0.6292216536100552 0.5992262854884399\n",
      "Saving model\n",
      "Saved/birnnscrat_lstm_64_3_100.pth\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:11, 41.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 295.37425653850215\n",
      "model previously passed\n",
      "Running eval on  train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:00, 565.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.78\n",
      " Fscore: 0.78\n",
      " Precision: 0.78\n",
      " Recall: 0.77\n",
      " Roc Auc: 0.92\n",
      " Test took: 0:00:01\n",
      "model previously passed\n",
      "Running eval on  val ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 549.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.65\n",
      " Fscore: 0.64\n",
      " Precision: 0.65\n",
      " Recall: 0.63\n",
      " Roc Auc: 0.81\n",
      " Test took: 0:00:00\n",
      "model previously passed\n",
      "Running eval on  test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 576.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.64\n",
      " Fscore: 0.63\n",
      " Precision: 0.64\n",
      " Recall: 0.63\n",
      " Roc Auc: 0.80\n",
      " Test took: 0:00:00\n",
      "  Test - fscore: 0.6332, accuracy: 0.6435\n",
      "  Val  - fscore: 0.6400, accuracy: 0.6504\n",
      "  Train- fscore: 0.7758, accuracy: 0.7817\n",
      "0.639963820145986 0.6292216536100552\n",
      "Saving model\n",
      "Saved/birnnscrat_lstm_64_3_100.pth\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:11, 41.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 295.3143782585921\n",
      "model previously passed\n",
      "Running eval on  train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [00:00, 552.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.76\n",
      " Fscore: 0.75\n",
      " Precision: 0.78\n",
      " Recall: 0.74\n",
      " Roc Auc: 0.92\n",
      " Test took: 0:00:01\n",
      "model previously passed\n",
      "Running eval on  val ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 587.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.60\n",
      " Fscore: 0.57\n",
      " Precision: 0.62\n",
      " Recall: 0.57\n",
      " Roc Auc: 0.77\n",
      " Test took: 0:00:00\n",
      "model previously passed\n",
      "Running eval on  test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 563.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.60\n",
      " Fscore: 0.57\n",
      " Precision: 0.62\n",
      " Recall: 0.56\n",
      " Roc Auc: 0.76\n",
      " Test took: 0:00:00\n",
      "  Test - fscore: 0.5673, accuracy: 0.5951\n",
      "  Val  - fscore: 0.5701, accuracy: 0.5968\n",
      "  Train- fscore: 0.7524, accuracy: 0.7645\n",
      "best_val_fscore 0.639963820145986\n",
      "best_test_fscore 0.6332487108352621\n",
      "best_val_rocauc 0.8071006833165059\n",
      "best_test_rocauc 0.8039198584224749\n",
      "best_val_precision 0.6517608919125689\n",
      "best_test_precision 0.6419289825978067\n",
      "best_val_recall 0.6348165172902596\n",
      "best_test_recall 0.6290887648657669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with 3 classes (hatespeech, offensive, normal)\n",
    "params['num_classes'] = 3\n",
    "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "if params['num_classes'] == 2 and params['auto_weights'] == False:\n",
    "    params['weights'] = [1.0, 1.0]\n",
    "\n",
    "print(f\"Training {params['num_classes']}-class model...\")\n",
    "train_model(params, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d53f6556",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d53f6556",
    "outputId": "c7120ee7-517b-457a-c3fe-db47502d732c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6620763",
   "metadata": {
    "id": "a6620763"
   },
   "source": [
    "## 4. Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7119f203",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7119f203",
    "outputId": "3dc13cd8-ee6a-497a-a131-647c17d2ba28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-07 13:07:47.651728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765112867.693933   34702 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765112867.705516   34702 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765112867.745428   34702 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765112867.745474   34702 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765112867.745483   34702 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765112867.745490   34702 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/usr/local/lib/python3.12/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "/usr/local/lib/python3.12/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
      "Reading english - 1grams ...\n",
      "There are 1 GPU(s) available.\n",
      "Found a gpu\n",
      "We will use the GPU: 0 Tesla T4\n",
      "total_data 1142\n",
      "100% 1142/1142 [00:01<00:00, 683.25it/s]\n",
      "100% 1142/1142 [00:00<00:00, 21147.44it/s]\n",
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n",
      "Saved/birnnscrat_lstm_64_3_100.pth\n",
      "Running eval on test data...\n",
      "100% 36/36 [00:00<00:00, 80.00it/s]\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      " Accuracy: 0.578\n",
      " Fscore: 0.442\n",
      " Precision: 0.525\n",
      " Recall: 0.383\n",
      " Test took: 0:00:00\n",
      "100% 1142/1142 [00:00<00:00, 15356.90it/s]\n",
      "There are 1 GPU(s) available.\n",
      "Found a gpu\n",
      "We will use the GPU: 0 Tesla T4\n",
      "100% 1142/1142 [00:00<00:00, 14760.76it/s]\n",
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n",
      "Saved/birnnscrat_lstm_64_3_100.pth\n",
      "Running eval on test data...\n",
      "100% 36/36 [00:00<00:00, 321.66it/s]\n",
      "100% 1142/1142 [00:00<00:00, 9660.20it/s]\n",
      "There are 1 GPU(s) available.\n",
      "Found a gpu\n",
      "We will use the GPU: 0 Tesla T4\n",
      "100% 1142/1142 [00:00<00:00, 13006.02it/s]\n",
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n",
      "Saved/birnnscrat_lstm_64_3_100.pth\n",
      "Running eval on test data...\n",
      "100% 36/36 [00:00<00:00, 416.70it/s]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Run testing scripts\n",
    "!python testing_with_rational.py birnn_scrat 100\n",
    "# !python testing_for_bias.py birnn_scrat 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb37bfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fb37bfd",
    "outputId": "fd4c5f39-978e-4a3f-c3bf-131ae826cd29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestModel_birnnscrat_100_explanation_top5.json\n"
     ]
    }
   ],
   "source": [
    "# Check generated explanation files\n",
    "!ls explanations_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805472e",
   "metadata": {
    "id": "e805472e"
   },
   "source": [
    "---\n",
    "\n",
    "# Bias Calculation\n",
    "\n",
    "Based on: Borkan et al. (2019) - \"Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd70e916",
   "metadata": {
    "id": "cd70e916"
   },
   "outputs": [],
   "source": [
    "# Import required libraries for bias calculation\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae89aa08",
   "metadata": {
    "id": "ae89aa08"
   },
   "outputs": [],
   "source": [
    "# Import data collection utilities\n",
    "from Preprocess.dataCollect import get_annotated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b7497b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b7497b6",
    "outputId": "0d66f70a-1fdd-4f28-ec25-c38fe68faec9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20148 samples\n"
     ]
    }
   ],
   "source": [
    "# Configure data loading for 2-class (toxic/non-toxic)\n",
    "dict_data_folder = {\n",
    "    '2': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes_two.npy'},\n",
    "    '3': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes.npy'}\n",
    "}\n",
    "\n",
    "params = {}\n",
    "params['num_classes'] = 2  # toxic vs non-toxic\n",
    "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "# Load the annotated dataset\n",
    "data_all_labelled = get_annotated_data(params)\n",
    "print(f\"Loaded {len(data_all_labelled)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ddf105",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "c9ddf105",
    "outputId": "799bd61b-6397-4844-dabe-e2bd650352cb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"data_all_labelled\",\n  \"rows\": 20148,\n  \"fields\": [\n    {\n      \"column\": \"post_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20148,\n        \"samples\": [\n          \"21376810_gab\",\n          \"1080843707643944965_twitter\",\n          \"1178183797461811200_twitter\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annotatorid1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95,\n        \"min\": 1,\n        \"max\": 251,\n        \"num_unique_values\": 194,\n        \"samples\": [\n          165,\n          41,\n          215\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target1\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"normal\",\n          \"hatespeech\",\n          \"offensive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annotatorid2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96,\n        \"min\": 1,\n        \"max\": 253,\n        \"num_unique_values\": 197,\n        \"samples\": [\n          173,\n          140,\n          37\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target2\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"normal\",\n          \"offensive\",\n          \"hatespeech\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annotatorid3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96,\n        \"min\": 1,\n        \"max\": 252,\n        \"num_unique_values\": 198,\n        \"samples\": [\n          96,\n          125,\n          33\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target3\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label3\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"normal\",\n          \"hatespeech\",\n          \"offensive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rationales\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"final_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"non-toxic\",\n          \"toxic\",\n          \"undecided\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "data_all_labelled"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-d784f806-ca91-487d-9885-e2a640ba2e64\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotatorid1</th>\n",
       "      <th>target1</th>\n",
       "      <th>label1</th>\n",
       "      <th>annotatorid2</th>\n",
       "      <th>target2</th>\n",
       "      <th>label2</th>\n",
       "      <th>annotatorid3</th>\n",
       "      <th>target3</th>\n",
       "      <th>label3</th>\n",
       "      <th>rationales</th>\n",
       "      <th>final_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>[]</td>\n",
       "      <td>non-toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>[we, cannot, continue, calling, ourselves, fem...</td>\n",
       "      <td>1</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>[]</td>\n",
       "      <td>non-toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>[nawt, yall, niggers, ignoring, me]</td>\n",
       "      <td>4</td>\n",
       "      <td>[African]</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>[African]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[]</td>\n",
       "      <td>non-toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>[&lt;user&gt;, i, am, bit, confused, coz, chinese, p...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>4</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>offensive</td>\n",
       "      <td>3</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Caucasian, Women]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>2</td>\n",
       "      <td>[Women, Caucasian]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>3</td>\n",
       "      <td>[Women, Caucasian]</td>\n",
       "      <td>offensive</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d784f806-ca91-487d-9885-e2a640ba2e64')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d784f806-ca91-487d-9885-e2a640ba2e64 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d784f806-ca91-487d-9885-e2a640ba2e64');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-89747853-b840-4fc9-ac29-974628547b2b\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-89747853-b840-4fc9-ac29-974628547b2b')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-89747853-b840-4fc9-ac29-974628547b2b button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                       post_id  \\\n",
       "0  1179055004553900032_twitter   \n",
       "1  1179063826874032128_twitter   \n",
       "2  1178793830532956161_twitter   \n",
       "3  1179088797964763136_twitter   \n",
       "4  1179085312976445440_twitter   \n",
       "\n",
       "                                                text  annotatorid1  \\\n",
       "0  [i, dont, think, im, getting, my, baby, them, ...             1   \n",
       "1  [we, cannot, continue, calling, ourselves, fem...             1   \n",
       "2                [nawt, yall, niggers, ignoring, me]             4   \n",
       "3  [<user>, i, am, bit, confused, coz, chinese, p...             1   \n",
       "4  [this, bitch, in, whataburger, eating, a, burg...             4   \n",
       "\n",
       "              target1      label1  annotatorid2             target2  \\\n",
       "0              [None]      normal             2              [None]   \n",
       "1              [None]      normal             2              [None]   \n",
       "2           [African]      normal             2              [None]   \n",
       "3             [Asian]  hatespeech             4             [Asian]   \n",
       "4  [Caucasian, Women]  hatespeech             2  [Women, Caucasian]   \n",
       "\n",
       "       label2  annotatorid3             target3      label3  \\\n",
       "0      normal             3              [None]      normal   \n",
       "1      normal             3              [None]      normal   \n",
       "2      normal             3           [African]  hatespeech   \n",
       "3   offensive             3             [Asian]  hatespeech   \n",
       "4  hatespeech             3  [Women, Caucasian]   offensive   \n",
       "\n",
       "                                          rationales final_label  \n",
       "0                                                 []   non-toxic  \n",
       "1                                                 []   non-toxic  \n",
       "2                                                 []   non-toxic  \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic  \n",
       "4  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample data\n",
    "data_all_labelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc0b0a9f",
   "metadata": {
    "id": "dc0b0a9f"
   },
   "outputs": [],
   "source": [
    "def generate_target_information(dataset):\n",
    "    \"\"\"Extract target community based on majority voting among annotators.\"\"\"\n",
    "    final_target_output = defaultdict(list)\n",
    "    all_communities_selected = []\n",
    "\n",
    "    for each in dataset.iterrows():\n",
    "        # Combine all target communities from 3 annotators\n",
    "        all_targets = each[1]['target1'] + each[1]['target2'] + each[1]['target3']\n",
    "        community_dict = dict(Counter(all_targets))\n",
    "\n",
    "        # Select communities mentioned by at least 2 annotators\n",
    "        for key in community_dict:\n",
    "            if community_dict[key] > 1:\n",
    "                final_target_output[each[1]['post_id']].append(key)\n",
    "                all_communities_selected.append(key)\n",
    "\n",
    "        # If no majority, mark as 'None'\n",
    "        if each[1]['post_id'] not in final_target_output:\n",
    "            final_target_output[each[1]['post_id']].append('None')\n",
    "            all_communities_selected.append('None')\n",
    "\n",
    "    return final_target_output, all_communities_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "810386dc",
   "metadata": {
    "id": "810386dc"
   },
   "outputs": [],
   "source": [
    "# Generate target information\n",
    "target_information, all_communities_selected = generate_target_information(data_all_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "589a7c28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "589a7c28",
    "outputId": "91630efa-56fd-4ed5-a874-9655c0f181af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 communities: ['African', 'Islam', 'Jewish', 'Homosexual', 'Women', 'Refugee', 'Arab', 'Caucasian', 'Asian', 'Hispanic']\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 communities for bias calculation\n",
    "community_count_dict = Counter(all_communities_selected)\n",
    "\n",
    "# Remove 'None' and 'Other' from consideration\n",
    "community_count_dict.pop('None', None)\n",
    "community_count_dict.pop('Other', None)\n",
    "\n",
    "# Select top 10 communities\n",
    "list_selected_community = [community for community, value in community_count_dict.most_common(10)]\n",
    "print(f\"Top 10 communities: {list_selected_community}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02c85f4a",
   "metadata": {
    "id": "02c85f4a"
   },
   "outputs": [],
   "source": [
    "# Filter target information to only include top 10 communities\n",
    "final_target_information = {}\n",
    "for each in target_information:\n",
    "    temp = list(set(target_information[each]) & set(list_selected_community))\n",
    "    if len(temp) == 0:\n",
    "        final_target_information[each] = None\n",
    "    else:\n",
    "        final_target_information[each] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d537ba3a",
   "metadata": {
    "id": "d537ba3a"
   },
   "outputs": [],
   "source": [
    "# Add target category column to dataset\n",
    "data_all_labelled['final_target_category'] = data_all_labelled['post_id'].map(final_target_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dd8cd32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dd8cd32",
    "outputId": "abf671a7-dfaa-41cb-a824-ae1535e019fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples for bias evaluation: 1924\n"
     ]
    }
   ],
   "source": [
    "# Load test split IDs and filter data\n",
    "with open('./Data/post_id_divisions.json', 'r') as fp:\n",
    "    post_id_dict = json.load(fp)\n",
    "\n",
    "data_all_labelled_bias = data_all_labelled[data_all_labelled['post_id'].isin(post_id_dict['test'])]\n",
    "print(f\"Test samples for bias evaluation: {len(data_all_labelled_bias)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14ae2de9",
   "metadata": {
    "id": "14ae2de9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Bias score file mapping for the trained model\n",
    "bias_score_file_mapping = {\n",
    "    'BiRNN-Attn': 'bestModel_birnnscrat_bias.json',\n",
    "}\n",
    "\n",
    "parent_path = './explanations_dicts/'\n",
    "method_list = ['subgroup', 'bpsn', 'bnsp']\n",
    "community_list = list(list_selected_community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "757024c0",
   "metadata": {
    "id": "757024c0"
   },
   "outputs": [],
   "source": [
    "def convert_to_score(label_name, label_dict):\n",
    "    \"\"\"Convert classification to toxicity score [0-1].\"\"\"\n",
    "    if label_name == 'non-toxic':\n",
    "        return 1 - label_dict[label_name]\n",
    "    else:\n",
    "        return label_dict[label_name]\n",
    "\n",
    "\n",
    "def bias_evaluation_metric(dataset, method, community):\n",
    "    \"\"\"Divide IDs into positive/negative based on bias evaluation method.\"\"\"\n",
    "    positive_ids = []\n",
    "    negative_ids = []\n",
    "\n",
    "    for eachrow in dataset.iterrows():\n",
    "        if eachrow[1]['final_target_category'] is None:\n",
    "            continue\n",
    "\n",
    "        is_community = community in eachrow[1]['final_target_category']\n",
    "        is_toxic = eachrow[1]['final_label'] != 'non-toxic'\n",
    "\n",
    "        if method == 'subgroup':\n",
    "            if is_community:\n",
    "                if is_toxic:\n",
    "                    positive_ids.append(eachrow[1]['post_id'])\n",
    "                else:\n",
    "                    negative_ids.append(eachrow[1]['post_id'])\n",
    "        elif method == 'bpsn':\n",
    "            if is_community and not is_toxic:\n",
    "                negative_ids.append(eachrow[1]['post_id'])\n",
    "            elif not is_community and is_toxic:\n",
    "                positive_ids.append(eachrow[1]['post_id'])\n",
    "        elif method == 'bnsp':\n",
    "            if is_community and is_toxic:\n",
    "                positive_ids.append(eachrow[1]['post_id'])\n",
    "            elif not is_community and not is_toxic:\n",
    "                negative_ids.append(eachrow[1]['post_id'])\n",
    "        else:\n",
    "            print('Incorrect method selected!')\n",
    "\n",
    "    return {'positiveID': positive_ids, 'negativeID': negative_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41dd1847",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "4cc6b5f94bde460a8708adb88dcc229a",
      "14e5505320504066846f6559aef64755",
      "a9cfb29027044f2db18d66993bd157b9",
      "f92cef57ddcc461b9a29f515a91834d0",
      "2cd138ed46fd4f55ab369e41ac923128",
      "dcf60c3141924ab684ea0399e6d52cda",
      "dc195a1516db4a92b50aec5960f6702b",
      "e9a7b5ac4526474ba14c0a571cc38485",
      "fb34bd51472044fc843e93e8f32e6b56",
      "f1762f6d086a4aa58fcfcb8d6817fbd6",
      "153d36e8a45f4c9cb52460677324418c"
     ]
    },
    "id": "41dd1847",
    "outputId": "4889e6b1-cd48-4958-e30c-8ea569e7f9ff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc6b5f94bde460a8708adb88dcc229a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing models:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: ./explanations_dicts/bestModel_birnnscrat_bias.json not found. Run testing scripts first.\n"
     ]
    }
   ],
   "source": [
    "# Calculate bias scores\n",
    "final_bias_dictionary = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for each_model in tqdm(bias_score_file_mapping, desc=\"Processing models\"):\n",
    "    total_data = {}\n",
    "    filepath = parent_path + bias_score_file_mapping[each_model]\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: {filepath} not found. Run testing scripts first.\")\n",
    "        continue\n",
    "\n",
    "    with open(filepath) as fp:\n",
    "        for line in fp:\n",
    "            data = json.loads(line)\n",
    "            total_data[data['annotation_id']] = data\n",
    "\n",
    "    for each_method in method_list:\n",
    "        for each_community in community_list:\n",
    "            community_data = bias_evaluation_metric(data_all_labelled_bias, each_method, each_community)\n",
    "            truth_values = []\n",
    "            prediction_values = []\n",
    "\n",
    "            label_to_value = {'toxic': 1.0, 'non-toxic': 0.0}\n",
    "\n",
    "            for each in community_data['positiveID']:\n",
    "                if each in total_data:\n",
    "                    truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
    "                    prediction_values.append(convert_to_score(\n",
    "                        total_data[each]['classification'],\n",
    "                        total_data[each]['classification_scores']\n",
    "                    ))\n",
    "\n",
    "            for each in community_data['negativeID']:\n",
    "                if each in total_data:\n",
    "                    truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
    "                    prediction_values.append(convert_to_score(\n",
    "                        total_data[each]['classification'],\n",
    "                        total_data[each]['classification_scores']\n",
    "                    ))\n",
    "\n",
    "            if len(truth_values) > 0 and len(set(truth_values)) > 1:\n",
    "                roc_output_value = roc_auc_score(truth_values, prediction_values)\n",
    "                final_bias_dictionary[each_model][each_method][each_community] = roc_output_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2000d34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2000d34",
    "outputId": "e4b653ad-d930-4489-c976-000294e7b5dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bias Scores (Generalized Mean):\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate generalized mean of bias scores\n",
    "power_value = -5\n",
    "num_communities = len(community_list)\n",
    "\n",
    "print(\"\\nBias Scores (Generalized Mean):\")\n",
    "print(\"=\" * 50)\n",
    "for each_model in final_bias_dictionary:\n",
    "    for each_method in final_bias_dictionary[each_model]:\n",
    "        temp_value = []\n",
    "        for each_community in final_bias_dictionary[each_model][each_method]:\n",
    "            temp_value.append(pow(final_bias_dictionary[each_model][each_method][each_community], power_value))\n",
    "        if len(temp_value) > 0:\n",
    "            score = pow(np.sum(temp_value) / num_communities, 1 / power_value)\n",
    "            print(f\"{each_model} | {each_method}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed22331",
   "metadata": {
    "id": "2ed22331"
   },
   "source": [
    "---\n",
    "\n",
    "# Calculate Explainability\n",
    "\n",
    "Based on: DeYoung et al. (2020) - \"ERASER: A Benchmark to Evaluate Rationalized NLP Models\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88c76dd4",
   "metadata": {
    "id": "88c76dd4"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import more_itertools as mit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e9aa0c6",
   "metadata": {
    "id": "9e9aa0c6"
   },
   "outputs": [],
   "source": [
    "# Import preprocessing utilities\n",
    "from Preprocess.dataCollect import get_annotated_data\n",
    "from Preprocess.spanMatcher import returnMask\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57ef8972",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57ef8972",
    "outputId": "ba4e02f8-222b-4741-eabc-0f9f8e8fbe02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20148 samples for explainability evaluation\n"
     ]
    }
   ],
   "source": [
    "# Load 3-class dataset for explainability\n",
    "dict_data_folder = {\n",
    "    '2': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes_two.npy'},\n",
    "    '3': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes.npy'}\n",
    "}\n",
    "\n",
    "params = {}\n",
    "params['num_classes'] = 3  # hatespeech, offensive, normal\n",
    "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "data_all_labelled = get_annotated_data(params)\n",
    "print(f\"Loaded {len(data_all_labelled)} samples for explainability evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eddc527e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eddc527e",
    "outputId": "c8bfcd6d-ebc1-42cd-fdb7-f0d601d89bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Configure tokenization parameters\n",
    "params_data = {\n",
    "    'include_special': False,\n",
    "    'bert_tokens': False,  # Set True for BERT models\n",
    "    'type_attention': 'softmax',\n",
    "    'set_decay': 0.1,\n",
    "    'majority': 2,\n",
    "    'max_length': 128,\n",
    "    'variance': 10,\n",
    "    'window': 4,\n",
    "    'alpha': 0.5,\n",
    "    'p_value': 0.8,\n",
    "    'method': 'additive',\n",
    "    'decay': False,\n",
    "    'normalized': False,\n",
    "    'not_recollect': True,\n",
    "}\n",
    "\n",
    "# Initialize tokenizer\n",
    "if params_data['bert_tokens']:\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
    "else:\n",
    "    print('Using standard tokenizer...')\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ce975f4",
   "metadata": {
    "id": "9ce975f4"
   },
   "outputs": [],
   "source": [
    "def get_training_data(data):\n",
    "    \"\"\"Load dataset and extract token-wise rationales.\"\"\"\n",
    "    final_output = []\n",
    "    print(f'Processing {len(data)} samples...')\n",
    "\n",
    "    for index, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        annotation = row['final_label']\n",
    "        post_id = row['post_id']\n",
    "        annotation_list = [row['label1'], row['label2'], row['label3']]\n",
    "\n",
    "        if annotation != 'undecided':\n",
    "            tokens_all, attention_masks = returnMask(row, params_data, tokenizer)\n",
    "            final_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c18b4ce2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "ac0db9c8c2244bb49f2186d749a2af4d",
      "56808345a73944549f02d7ea5a634178",
      "992ef4e69aa74019b0b96fb9c3ebe1d3",
      "1432a74a5a97407fabbaa5f8f9b901cf",
      "1bafa0c98e0d4e61a2c16b02225f595c",
      "556a77ce07eb4c9e92aefb97c513e3ce",
      "5f2e016a93da47babdfedc353931de12",
      "244d1fee914745298b5acf6cf6237e55",
      "206636f1b64347a393f5435a967dcd9e",
      "a890a908b3b54e368c5db87395ac1448",
      "cae15a413af84489a9fd713137f619d3"
     ]
    },
    "id": "c18b4ce2",
    "outputId": "3fdf6490-3e2d-4dd5-d638-ffb220c7b5ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20148 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0db9c8c2244bb49f2186d749a2af4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 19229 valid samples\n"
     ]
    }
   ],
   "source": [
    "# Process training data\n",
    "training_data = get_training_data(data_all_labelled)\n",
    "print(f\"Processed {len(training_data)} valid samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71bb9b84",
   "metadata": {
    "id": "71bb9b84"
   },
   "outputs": [],
   "source": [
    "def find_ranges(iterable):\n",
    "    \"\"\"Yield ranges of consecutive numbers.\"\"\"\n",
    "    for group in mit.consecutive_groups(iterable):\n",
    "        group = list(group)\n",
    "        if len(group) == 1:\n",
    "            yield group[0]\n",
    "        else:\n",
    "            yield group[0], group[-1]\n",
    "\n",
    "\n",
    "def get_evidence(post_id, anno_text, explanations):\n",
    "    \"\"\"Convert explanations to ERASER evidence format.\"\"\"\n",
    "    output = []\n",
    "    indexes = sorted([i for i, each in enumerate(explanations) if each == 1])\n",
    "    span_list = list(find_ranges(indexes))\n",
    "\n",
    "    for each in span_list:\n",
    "        if isinstance(each, int):\n",
    "            start, end = each, each + 1\n",
    "        elif len(each) == 2:\n",
    "            start, end = each[0], each[1] + 1\n",
    "        else:\n",
    "            print('Error in span processing')\n",
    "            continue\n",
    "\n",
    "        output.append({\n",
    "            \"docid\": post_id,\n",
    "            \"end_sentence\": -1,\n",
    "            \"end_token\": end,\n",
    "            \"start_sentence\": -1,\n",
    "            \"start_token\": start,\n",
    "            \"text\": ' '.join([str(x) for x in anno_text[start:end]])\n",
    "        })\n",
    "    return output\n",
    "\n",
    "\n",
    "def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):\n",
    "    \"\"\"Convert dataset to ERASER benchmark format.\"\"\"\n",
    "    final_output = []\n",
    "\n",
    "    if save_split:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_path, 'docs'), exist_ok=True)\n",
    "        train_fp = open(os.path.join(save_path, 'train.jsonl'), 'w')\n",
    "        val_fp = open(os.path.join(save_path, 'val.jsonl'), 'w')\n",
    "        test_fp = open(os.path.join(save_path, 'test.jsonl'), 'w')\n",
    "\n",
    "    for eachrow in dataset:\n",
    "        post_id = eachrow[0]\n",
    "        post_class = eachrow[1]\n",
    "        anno_text_list = eachrow[2]\n",
    "\n",
    "        if post_class == 'normal':\n",
    "            continue\n",
    "\n",
    "        explanations = [list(each_explain) for each_explain in eachrow[3]]\n",
    "\n",
    "        # Union of explanations from all annotators\n",
    "        if method == 'union':\n",
    "            final_explanation = [int(any(each)) for each in zip(*explanations)]\n",
    "\n",
    "        temp = {\n",
    "            'annotation_id': post_id,\n",
    "            'classification': post_class,\n",
    "            'evidences': [get_evidence(post_id, list(anno_text_list), final_explanation)],\n",
    "            'query': \"What is the class?\",\n",
    "            'query_type': None\n",
    "        }\n",
    "        final_output.append(temp)\n",
    "\n",
    "        if save_split:\n",
    "            # Save document\n",
    "            with open(os.path.join(save_path, 'docs', post_id), 'w') as fp:\n",
    "                fp.write(' '.join([str(x) for x in list(anno_text_list)]))\n",
    "\n",
    "            # Save to appropriate split\n",
    "            if post_id in id_division['train']:\n",
    "                train_fp.write(json.dumps(temp) + '\\n')\n",
    "            elif post_id in id_division['val']:\n",
    "                val_fp.write(json.dumps(temp) + '\\n')\n",
    "            elif post_id in id_division['test']:\n",
    "                test_fp.write(json.dumps(temp) + '\\n')\n",
    "\n",
    "    if save_split:\n",
    "        train_fp.close()\n",
    "        val_fp.close()\n",
    "        test_fp.close()\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e8013a1",
   "metadata": {
    "id": "0e8013a1"
   },
   "outputs": [],
   "source": [
    "# Load data splits\n",
    "with open('./Data/post_id_divisions.json') as fp:\n",
    "    id_division = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80332ce4",
   "metadata": {
    "id": "80332ce4"
   },
   "outputs": [],
   "source": [
    "# Create evaluation directory\n",
    "os.makedirs('./Data/Evaluation/Model_Eval', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d0e7d11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d0e7d11",
    "outputId": "b09dae97-49c3-46bb-9ac3-c7f30179b0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 11415 samples to ERASER format\n"
     ]
    }
   ],
   "source": [
    "# Convert to ERASER format\n",
    "method = 'union'\n",
    "save_split = True\n",
    "save_path = './Data/Evaluation/Model_Eval/'\n",
    "\n",
    "output_eraser = convert_to_eraser_format(training_data, method, save_split, save_path, id_division)\n",
    "print(f\"Converted {len(output_eraser)} samples to ERASER format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b2f7035",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b2f7035",
    "outputId": "c35059d9-a6b9-4a23-9e04-245b8dddbe91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs  test.jsonl  train.jsonl  val.jsonl\n"
     ]
    }
   ],
   "source": [
    "# List generated files\n",
    "!ls Data/Evaluation/Model_Eval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c70dfea6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c70dfea6",
    "outputId": "6bbe310e-e648-48b8-d70a-9eada1485611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6039 MainThread Error in instances: 0 instances fail validation: set()\n",
      "  9510 MainThread No sentence level predictions detected, skipping sentence-level diagnostic\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "{'classification_scores': {'accuracy': 0.5779334500875657,\n",
      "                           'aopc_thresholds': None,\n",
      "                           'comprehensiveness': np.float64(0.3071190950170407),\n",
      "                           'comprehensiveness_aopc': None,\n",
      "                           'comprehensiveness_aopc_points': None,\n",
      "                           'comprehensiveness_entropy': np.float64(0.16702580912683954),\n",
      "                           'comprehensiveness_kl': np.float64(0.8087778324712184),\n",
      "                           'prf': {'accuracy': 0.5779334500875657,\n",
      "                                   'hatespeech': {'f1-score': 0.7516525023607177,\n",
      "                                                  'precision': 0.8559139784946237,\n",
      "                                                  'recall': 0.67003367003367,\n",
      "                                                  'support': 594.0},\n",
      "                                   'macro avg': {'f1-score': 0.4420713019564966,\n",
      "                                                 'precision': 0.5252313994249479,\n",
      "                                                 'recall': 0.38271195327156393,\n",
      "                                                 'support': 1142.0},\n",
      "                                   'normal': {'f1-score': 0.0,\n",
      "                                              'precision': 0.0,\n",
      "                                              'recall': 0.0,\n",
      "                                              'support': 0.0},\n",
      "                                   'offensive': {'f1-score': 0.5745614035087719,\n",
      "                                                 'precision': 0.7197802197802198,\n",
      "                                                 'recall': 0.4781021897810219,\n",
      "                                                 'support': 548.0},\n",
      "                                   'weighted avg': {'f1-score': 0.6666735862741447,\n",
      "                                                    'precision': 0.7905888473427031,\n",
      "                                                    'recall': 0.5779334500875657,\n",
      "                                                    'support': 1142.0}},\n",
      "                           'sufficiency': np.float64(0.043159904207988604),\n",
      "                           'sufficiency_aopc': None,\n",
      "                           'sufficiency_aopc_points': None,\n",
      "                           'sufficiency_entropy': np.float64(0.012302636693536505),\n",
      "                           'sufficiency_kl': np.float64(0.082762051427988)},\n",
      " 'iou_scores': [{'macro': {'f1': 0.2223972913201478,\n",
      "                           'p': 0.14403093987157037,\n",
      "                           'r': 0.48781377699941625},\n",
      "                 'micro': {'f1': 0.2202906424011951,\n",
      "                           'p': 0.14318502824858756,\n",
      "                           'r': 0.47733961153619775},\n",
      "                 'threshold': 0.5}],\n",
      " 'rationale_prf': {'instance_macro': {'f1': 0.11444852578828062,\n",
      "                                      'p': 0.07710157618213662,\n",
      "                                      'r': 0.2609311150029189},\n",
      "                   'instance_micro': {'f1': 0.11815835936438952,\n",
      "                                      'p': 0.07680084745762712,\n",
      "                                      'r': 0.25603296056503827}},\n",
      " 'token_prf': {'instance_macro': {'f1': 0.503974763156197,\n",
      "                                  'p': 0.6192498540572097,\n",
      "                                  'r': 0.6368745027505276},\n",
      "               'instance_micro': {'f1': 0.4430827542640556,\n",
      "                                  'p': 0.6191737288135594,\n",
      "                                  'r': 0.3449734408813693}},\n",
      " 'token_soft_metrics': {'auprc': np.float64(0.8412134937433974),\n",
      "                        'average_precision': np.float64(0.8360175423997078),\n",
      "                        'roc_auc_score': np.float64(0.8547483501545116)}}\n"
     ]
    }
   ],
   "source": [
    "# Run ERASER metrics\n",
    "explanation_file = './explanations_dicts/bestModel_birnnscrat_100_explanation_top5.json'\n",
    "if os.path.exists(explanation_file):\n",
    "    !cd eraserbenchmark && PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py \\\n",
    "        --split test \\\n",
    "        --data_dir ../Data/Evaluation/Model_Eval \\\n",
    "        --results ../explanations_dicts/bestModel_birnnscrat_100_explanation_top5.json \\\n",
    "        --score_file ../model_explain_output.json\n",
    "else:\n",
    "    print(f\"Explanation file not found: {explanation_file}\")\n",
    "    print(\"Run testing_with_rational.py first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2923617",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2923617",
    "outputId": "e4f1e4b9-9eea-4be4-8015-7052f613ccfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXPLAINABILITY RESULTS\n",
      "==================================================\n",
      "\n",
      "Plausibility:\n",
      "  IOU F1:   0.2224\n",
      "  Token F1: 0.5040\n",
      "  AUPRC:    0.8412\n",
      "\n",
      "Faithfulness:\n",
      "  Comprehensiveness: 0.3071\n",
      "  Sufficiency:       0.0432\n"
     ]
    }
   ],
   "source": [
    "# Print explainability results\n",
    "output_file = './model_explain_output.json'\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file) as fp:\n",
    "        output_data = json.load(fp)\n",
    "\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('EXPLAINABILITY RESULTS')\n",
    "    print('=' * 50)\n",
    "\n",
    "    print('\\nPlausibility:')\n",
    "    print(f\"  IOU F1:   {output_data['iou_scores'][0]['macro']['f1']:.4f}\")\n",
    "    print(f\"  Token F1: {output_data['token_prf']['instance_macro']['f1']:.4f}\")\n",
    "    print(f\"  AUPRC:    {output_data['token_soft_metrics']['auprc']:.4f}\")\n",
    "\n",
    "    print('\\nFaithfulness:')\n",
    "    print(f\"  Comprehensiveness: {output_data['classification_scores']['comprehensiveness']:.4f}\")\n",
    "    print(f\"  Sufficiency:       {output_data['classification_scores']['sufficiency']:.4f}\")\n",
    "else:\n",
    "    print(f\"Output file not found: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a912fa",
   "metadata": {
    "id": "98a912fa"
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Model Training**: Training BiRNN-SCRAT model for hate speech detection\n",
    "2. **Bias Evaluation**: Computing subgroup, BPSN, and BNSP bias metrics\n",
    "3. **Explainability Evaluation**: Computing plausibility and faithfulness metrics\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1432a74a5a97407fabbaa5f8f9b901cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a890a908b3b54e368c5db87395ac1448",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cae15a413af84489a9fd713137f619d3",
      "value": "‚Äá20148/20148‚Äá[00:32&lt;00:00,‚Äá748.41it/s]"
     }
    },
    "14e5505320504066846f6559aef64755": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dcf60c3141924ab684ea0399e6d52cda",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_dc195a1516db4a92b50aec5960f6702b",
      "value": "Processing‚Äámodels:‚Äá100%"
     }
    },
    "153d36e8a45f4c9cb52460677324418c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1bafa0c98e0d4e61a2c16b02225f595c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "206636f1b64347a393f5435a967dcd9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "244d1fee914745298b5acf6cf6237e55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cd138ed46fd4f55ab369e41ac923128": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cc6b5f94bde460a8708adb88dcc229a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14e5505320504066846f6559aef64755",
       "IPY_MODEL_a9cfb29027044f2db18d66993bd157b9",
       "IPY_MODEL_f92cef57ddcc461b9a29f515a91834d0"
      ],
      "layout": "IPY_MODEL_2cd138ed46fd4f55ab369e41ac923128"
     }
    },
    "556a77ce07eb4c9e92aefb97c513e3ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56808345a73944549f02d7ea5a634178": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_556a77ce07eb4c9e92aefb97c513e3ce",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5f2e016a93da47babdfedc353931de12",
      "value": "100%"
     }
    },
    "5f2e016a93da47babdfedc353931de12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "992ef4e69aa74019b0b96fb9c3ebe1d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_244d1fee914745298b5acf6cf6237e55",
      "max": 20148,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_206636f1b64347a393f5435a967dcd9e",
      "value": 20148
     }
    },
    "a890a908b3b54e368c5db87395ac1448": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9cfb29027044f2db18d66993bd157b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9a7b5ac4526474ba14c0a571cc38485",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fb34bd51472044fc843e93e8f32e6b56",
      "value": 1
     }
    },
    "ac0db9c8c2244bb49f2186d749a2af4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56808345a73944549f02d7ea5a634178",
       "IPY_MODEL_992ef4e69aa74019b0b96fb9c3ebe1d3",
       "IPY_MODEL_1432a74a5a97407fabbaa5f8f9b901cf"
      ],
      "layout": "IPY_MODEL_1bafa0c98e0d4e61a2c16b02225f595c"
     }
    },
    "cae15a413af84489a9fd713137f619d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc195a1516db4a92b50aec5960f6702b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dcf60c3141924ab684ea0399e6d52cda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9a7b5ac4526474ba14c0a571cc38485": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1762f6d086a4aa58fcfcb8d6817fbd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f92cef57ddcc461b9a29f515a91834d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1762f6d086a4aa58fcfcb8d6817fbd6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_153d36e8a45f4c9cb52460677324418c",
      "value": "‚Äá1/1‚Äá[00:00&lt;00:00,‚Äá91.14it/s]"
     }
    },
    "fb34bd51472044fc843e93e8f32e6b56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
