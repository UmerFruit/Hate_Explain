{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# GOOGLE COLAB SETUP - Mount Drive & Extract Data\n",
        "# ================================================\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/UmerFruit/Hate_Explain.git\n",
        "%cd Hate_Explain\n",
        "print(\"DONE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EGDu9Wbvnm2",
        "outputId": "26a03a0d-299b-4dd5-ce9f-dc4a4e43cbec"
      },
      "id": "7EGDu9Wbvnm2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "fatal: destination path 'Hate_Explain' already exists and is not an empty directory.\n",
            "/content/Hate_Explain\n",
            "DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a557a518",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a557a518",
        "outputId": "623c20a6-13cd-45a4-bd96-f73501f4f5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPU Available: True\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# LOAD THE SMALL DATASET (GIVES KEY ERRORS SINCE IT DOESNT HAVE THE MOST OF THE WORDS)\n",
        "# !cp /content/drive/MyDrive/glove.42B.300d.small.zip ./Data/\n",
        "# Extract the zip file\n",
        "# !unzip -q ./Data/glove.42B.300d.small.zip -d ./Data/\n",
        "# !mv ./Data/glove.42B.300d.small.txt  ./Data/glove.42B.300d.txt\n",
        "\n",
        "# LOAD THE FULL DATASET\n",
        "print(\"Starting copy\")\n",
        "!cp /content/drive/MyDrive/glove.42B.300d.zip ./Data/\n",
        "# Extract the zip file\n",
        "print(\"Starting extraction\")\n",
        "!unzip -q ./Data/glove.42B.300d.zip -d ./Data/\n",
        "print(\"Done extraction\")\n",
        "\n",
        "# Clean up zip file (if needed)\n",
        "# !rm ./Data/glove.42B.300d.small.zip\n",
        "# !rm ./Data/glove.42B.300d.zip\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"\\nGPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b64eb58",
      "metadata": {
        "id": "7b64eb58"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c630f3f1",
      "metadata": {
        "id": "c630f3f1"
      },
      "outputs": [],
      "source": [
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ed11d7bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed11d7bc",
        "outputId": "7589f9bd-3353-4d45-9764-d716d1a3adae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create necessary directories\n",
        "import os\n",
        "os.makedirs('Saved', exist_ok=True)\n",
        "os.makedirs('explanations_dicts', exist_ok=True)\n",
        "print(\"Directories created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e725d768",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e725d768",
        "outputId": "1ee6dbac-08c5-4ed1-c1a1-221e03829963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.57.2)\n",
            "Requirement already satisfied: scipy>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (1.16.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
            "Requirement already satisfied: spacy>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.8.11)\n",
            "Requirement already satisfied: gensim>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (4.4.0)\n",
            "Requirement already satisfied: ekphrasis>=0.5.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.5.4)\n",
            "Requirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.66.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (4.67.1)\n",
            "Requirement already satisfied: lime>=0.2.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 19)) (0.2.0.1)\n",
            "Requirement already satisfied: GPUtil>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 21)) (10.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.0->-r requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.0->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.0->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 7)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (0.20.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.7.0->-r requirements.txt (line 10)) (2.12.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim>=4.3.0->-r requirements.txt (line 11)) (7.5.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from ekphrasis>=0.5.4->-r requirements.txt (line 12)) (3.2.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from ekphrasis>=0.5.4->-r requirements.txt (line 12)) (0.4.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.12/dist-packages (from ekphrasis>=0.5.4->-r requirements.txt (line 12)) (5.11.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from ekphrasis>=0.5.4->-r requirements.txt (line 12)) (3.9.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from ekphrasis>=0.5.4->-r requirements.txt (line 12)) (6.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 15)) (3.2.5)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime>=0.2.0.1->-r requirements.txt (line 19)) (0.25.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.7.0->-r requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.7.0->-r requirements.txt (line 10)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.7.0->-r requirements.txt (line 10)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.36.0->-r requirements.txt (line 3)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.36.0->-r requirements.txt (line 3)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.36.0->-r requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.36.0->-r requirements.txt (line 3)) (2025.11.12)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime>=0.2.0.1->-r requirements.txt (line 19)) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime>=0.2.0.1->-r requirements.txt (line 19)) (2025.10.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime>=0.2.0.1->-r requirements.txt (line 19)) (0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim>=4.3.0->-r requirements.txt (line 11)) (2.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.7.0->-r requirements.txt (line 10)) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.7.0->-r requirements.txt (line 10)) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy>=3.7.0->-r requirements.txt (line 10)) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy>=3.7.0->-r requirements.txt (line 10)) (0.23.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->ekphrasis>=0.5.4->-r requirements.txt (line 12)) (0.2.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->-r requirements.txt (line 2)) (3.0.3)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Install required packages (run this if not already installed)\n",
        "!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "931f2403",
      "metadata": {
        "id": "931f2403"
      },
      "source": [
        "## 2. Download and Prepare GloVe Embeddings\n",
        "\n",
        "**Note:** This step is only required once. Skip if you already have the file. Like i did with in the google drive mounted. If not it downloads it right here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ca9c64cf",
      "metadata": {
        "id": "ca9c64cf"
      },
      "outputs": [],
      "source": [
        "# Download GloVe embeddings (only run if needed)\n",
        "# !wget http://nlp.stanford.edu/data/glove.42B.300d.zip -P Data/\n",
        "# !unzip Data/glove.42B.300d.zip -d Data/\n",
        "# !rm Data/glove.42B.300d.zip\n",
        "# print(\"GloVe embeddings downloaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "044d03bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "044d03bc",
        "outputId": "44300541-172f-4720-9d97-e76120ec8725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting GloVe to Word2Vec format...\n"
          ]
        }
      ],
      "source": [
        "# Convert GloVe to Word2Vec format (REQUIRED for Colab - run this!)\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# Convert GloVe format to Word2Vec format\n",
        "print(\"Converting GloVe to Word2Vec format...\")\n",
        "glove2word2vec('Data/glove.42B.300d.txt', 'Data/glove.42B.300d_w2v.txt')\n",
        "\n",
        "# Load and save in gensim format\n",
        "print(\"Loading and saving model (this may take a few minutes)...\")\n",
        "word2vecmodel1 = KeyedVectors.load_word2vec_format('Data/glove.42B.300d_w2v.txt', binary=False)\n",
        "word2vecmodel1.save(\"Data/word2vec.model\")\n",
        "\n",
        "# Clean up intermediate files\n",
        "import gc\n",
        "del word2vecmodel1\n",
        "gc.collect()\n",
        "\n",
        "# Remove large text files to save space\n",
        "import os\n",
        "os.remove('Data/glove.42B.300d.txt')\n",
        "os.remove('Data/glove.42B.300d_w2v.txt')\n",
        "print(\"Done! word2vec.model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7938582a",
      "metadata": {
        "id": "7938582a"
      },
      "source": [
        "## 3. Import Dependencies and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ff1eed7",
      "metadata": {
        "id": "7ff1eed7"
      },
      "outputs": [],
      "source": [
        "# Import the training module\n",
        "from manual_training_inference import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9079f11",
      "metadata": {
        "id": "e9079f11"
      },
      "outputs": [],
      "source": [
        "# Load model parameters from JSON configuration\n",
        "import json\n",
        "import ast\n",
        "import torch\n",
        "\n",
        "path_file = 'best_model_json/bestModel_birnnscrat.json'\n",
        "with open(path_file, mode='r') as f:\n",
        "    params = json.load(f)\n",
        "\n",
        "# Convert string values to appropriate types\n",
        "for key in params:\n",
        "    if params[key] == 'True':\n",
        "        params[key] = True\n",
        "    elif params[key] == 'False':\n",
        "        params[key] = False\n",
        "    if key in ['batch_size', 'num_classes', 'hidden_size', 'supervised_layer_pos',\n",
        "               'num_supervised_heads', 'random_seed', 'max_length']:\n",
        "        if params[key] != 'N/A':\n",
        "            params[key] = int(params[key])\n",
        "    if (key == 'weights') and (params['auto_weights'] == False):\n",
        "        params[key] = ast.literal_eval(params[key])\n",
        "\n",
        "# Configure for Colab execution\n",
        "params['logging'] = 'local'\n",
        "params['device'] = 'cuda'  # Use GPU in Colab\n",
        "params['best_params'] = False\n",
        "\n",
        "# Setup device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    print('WARNING: GPU not available. Using CPU (training will be slow).')\n",
        "    print('Go to Runtime → Change runtime type → GPU')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d7f249",
      "metadata": {
        "id": "01d7f249"
      },
      "outputs": [],
      "source": [
        "# Data folder configuration\n",
        "dict_data_folder = {\n",
        "    '2': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes_two.npy'},\n",
        "    '3': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "# Configure training parameters\n",
        "params['variance'] = 1\n",
        "params['epochs'] = 5  # Reduce for faster testing\n",
        "params['to_save'] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d1e4a47",
      "metadata": {
        "id": "0d1e4a47"
      },
      "outputs": [],
      "source": [
        "# Train with 2 classes (toxic vs non-toxic)\n",
        "params['num_classes'] = 2\n",
        "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "if params['num_classes'] == 2 and params['auto_weights'] == False:\n",
        "    params['weights'] = [1.0, 1.0]\n",
        "\n",
        "print(f\"Training {params['num_classes']}-class model...\")\n",
        "train_model(params, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0fc73b",
      "metadata": {
        "id": "cf0fc73b"
      },
      "outputs": [],
      "source": [
        "# Train with 3 classes (hatespeech, offensive, normal)\n",
        "params['num_classes'] = 3\n",
        "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "if params['num_classes'] == 2 and params['auto_weights'] == False:\n",
        "    params['weights'] = [1.0, 1.0]\n",
        "\n",
        "print(f\"Training {params['num_classes']}-class model...\")\n",
        "train_model(params, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d53f6556",
      "metadata": {
        "id": "d53f6556"
      },
      "outputs": [],
      "source": [
        "# Clean up memory\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6620763",
      "metadata": {
        "id": "a6620763"
      },
      "source": [
        "## 4. Testing and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7119f203",
      "metadata": {
        "id": "7119f203"
      },
      "outputs": [],
      "source": [
        "# Run testing scripts\n",
        "!python testing_with_rational.py birnn_scrat 100\n",
        "!python testing_for_bias.py birnn_scrat 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fb37bfd",
      "metadata": {
        "id": "9fb37bfd"
      },
      "outputs": [],
      "source": [
        "# Check generated explanation files\n",
        "!ls explanations_dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e805472e",
      "metadata": {
        "id": "e805472e"
      },
      "source": [
        "---\n",
        "\n",
        "# Bias Calculation\n",
        "\n",
        "Based on: Borkan et al. (2019) - \"Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification\"\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd70e916",
      "metadata": {
        "id": "cd70e916"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for bias calculation\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae89aa08",
      "metadata": {
        "id": "ae89aa08"
      },
      "outputs": [],
      "source": [
        "# Import data collection utilities\n",
        "from Preprocess.dataCollect import get_annotated_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b7497b6",
      "metadata": {
        "id": "7b7497b6"
      },
      "outputs": [],
      "source": [
        "# Configure data loading for 2-class (toxic/non-toxic)\n",
        "dict_data_folder = {\n",
        "    '2': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes_two.npy'},\n",
        "    '3': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "params = {}\n",
        "params['num_classes'] = 2  # toxic vs non-toxic\n",
        "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "# Load the annotated dataset\n",
        "data_all_labelled = get_annotated_data(params)\n",
        "print(f\"Loaded {len(data_all_labelled)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9ddf105",
      "metadata": {
        "id": "c9ddf105"
      },
      "outputs": [],
      "source": [
        "# Display sample data\n",
        "data_all_labelled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0b0a9f",
      "metadata": {
        "id": "dc0b0a9f"
      },
      "outputs": [],
      "source": [
        "def generate_target_information(dataset):\n",
        "    \"\"\"Extract target community based on majority voting among annotators.\"\"\"\n",
        "    final_target_output = defaultdict(list)\n",
        "    all_communities_selected = []\n",
        "\n",
        "    for each in dataset.iterrows():\n",
        "        # Combine all target communities from 3 annotators\n",
        "        all_targets = each[1]['target1'] + each[1]['target2'] + each[1]['target3']\n",
        "        community_dict = dict(Counter(all_targets))\n",
        "\n",
        "        # Select communities mentioned by at least 2 annotators\n",
        "        for key in community_dict:\n",
        "            if community_dict[key] > 1:\n",
        "                final_target_output[each[1]['post_id']].append(key)\n",
        "                all_communities_selected.append(key)\n",
        "\n",
        "        # If no majority, mark as 'None'\n",
        "        if each[1]['post_id'] not in final_target_output:\n",
        "            final_target_output[each[1]['post_id']].append('None')\n",
        "            all_communities_selected.append('None')\n",
        "\n",
        "    return final_target_output, all_communities_selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "810386dc",
      "metadata": {
        "id": "810386dc"
      },
      "outputs": [],
      "source": [
        "# Generate target information\n",
        "target_information, all_communities_selected = generate_target_information(data_all_labelled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589a7c28",
      "metadata": {
        "id": "589a7c28"
      },
      "outputs": [],
      "source": [
        "# Get top 10 communities for bias calculation\n",
        "community_count_dict = Counter(all_communities_selected)\n",
        "\n",
        "# Remove 'None' and 'Other' from consideration\n",
        "community_count_dict.pop('None', None)\n",
        "community_count_dict.pop('Other', None)\n",
        "\n",
        "# Select top 10 communities\n",
        "list_selected_community = [community for community, value in community_count_dict.most_common(10)]\n",
        "print(f\"Top 10 communities: {list_selected_community}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c85f4a",
      "metadata": {
        "id": "02c85f4a"
      },
      "outputs": [],
      "source": [
        "# Filter target information to only include top 10 communities\n",
        "final_target_information = {}\n",
        "for each in target_information:\n",
        "    temp = list(set(target_information[each]) & set(list_selected_community))\n",
        "    if len(temp) == 0:\n",
        "        final_target_information[each] = None\n",
        "    else:\n",
        "        final_target_information[each] = temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d537ba3a",
      "metadata": {
        "id": "d537ba3a"
      },
      "outputs": [],
      "source": [
        "# Add target category column to dataset\n",
        "data_all_labelled['final_target_category'] = data_all_labelled['post_id'].map(final_target_information)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dd8cd32",
      "metadata": {
        "id": "7dd8cd32"
      },
      "outputs": [],
      "source": [
        "# Load test split IDs and filter data\n",
        "with open('./Data/post_id_divisions.json', 'r') as fp:\n",
        "    post_id_dict = json.load(fp)\n",
        "\n",
        "data_all_labelled_bias = data_all_labelled[data_all_labelled['post_id'].isin(post_id_dict['test'])]\n",
        "print(f\"Test samples for bias evaluation: {len(data_all_labelled_bias)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14ae2de9",
      "metadata": {
        "id": "14ae2de9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Bias score file mapping for the trained model\n",
        "bias_score_file_mapping = {\n",
        "    'BiRNN-Attn': 'bestModel_birnnscrat_bias.json',\n",
        "}\n",
        "\n",
        "parent_path = './explanations_dicts/'\n",
        "method_list = ['subgroup', 'bpsn', 'bnsp']\n",
        "community_list = list(list_selected_community)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "757024c0",
      "metadata": {
        "id": "757024c0"
      },
      "outputs": [],
      "source": [
        "def convert_to_score(label_name, label_dict):\n",
        "    \"\"\"Convert classification to toxicity score [0-1].\"\"\"\n",
        "    if label_name == 'non-toxic':\n",
        "        return 1 - label_dict[label_name]\n",
        "    else:\n",
        "        return label_dict[label_name]\n",
        "\n",
        "\n",
        "def bias_evaluation_metric(dataset, method, community):\n",
        "    \"\"\"Divide IDs into positive/negative based on bias evaluation method.\"\"\"\n",
        "    positive_ids = []\n",
        "    negative_ids = []\n",
        "\n",
        "    for eachrow in dataset.iterrows():\n",
        "        if eachrow[1]['final_target_category'] is None:\n",
        "            continue\n",
        "\n",
        "        is_community = community in eachrow[1]['final_target_category']\n",
        "        is_toxic = eachrow[1]['final_label'] != 'non-toxic'\n",
        "\n",
        "        if method == 'subgroup':\n",
        "            if is_community:\n",
        "                if is_toxic:\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "        elif method == 'bpsn':\n",
        "            if is_community and not is_toxic:\n",
        "                negative_ids.append(eachrow[1]['post_id'])\n",
        "            elif not is_community and is_toxic:\n",
        "                positive_ids.append(eachrow[1]['post_id'])\n",
        "        elif method == 'bnsp':\n",
        "            if is_community and is_toxic:\n",
        "                positive_ids.append(eachrow[1]['post_id'])\n",
        "            elif not is_community and not is_toxic:\n",
        "                negative_ids.append(eachrow[1]['post_id'])\n",
        "        else:\n",
        "            print('Incorrect method selected!')\n",
        "\n",
        "    return {'positiveID': positive_ids, 'negativeID': negative_ids}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41dd1847",
      "metadata": {
        "id": "41dd1847"
      },
      "outputs": [],
      "source": [
        "# Calculate bias scores\n",
        "final_bias_dictionary = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "for each_model in tqdm(bias_score_file_mapping, desc=\"Processing models\"):\n",
        "    total_data = {}\n",
        "    filepath = parent_path + bias_score_file_mapping[each_model]\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Warning: {filepath} not found. Run testing scripts first.\")\n",
        "        continue\n",
        "\n",
        "    with open(filepath) as fp:\n",
        "        for line in fp:\n",
        "            data = json.loads(line)\n",
        "            total_data[data['annotation_id']] = data\n",
        "\n",
        "    for each_method in method_list:\n",
        "        for each_community in community_list:\n",
        "            community_data = bias_evaluation_metric(data_all_labelled_bias, each_method, each_community)\n",
        "            truth_values = []\n",
        "            prediction_values = []\n",
        "\n",
        "            label_to_value = {'toxic': 1.0, 'non-toxic': 0.0}\n",
        "\n",
        "            for each in community_data['positiveID']:\n",
        "                if each in total_data:\n",
        "                    truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                    prediction_values.append(convert_to_score(\n",
        "                        total_data[each]['classification'],\n",
        "                        total_data[each]['classification_scores']\n",
        "                    ))\n",
        "\n",
        "            for each in community_data['negativeID']:\n",
        "                if each in total_data:\n",
        "                    truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                    prediction_values.append(convert_to_score(\n",
        "                        total_data[each]['classification'],\n",
        "                        total_data[each]['classification_scores']\n",
        "                    ))\n",
        "\n",
        "            if len(truth_values) > 0 and len(set(truth_values)) > 1:\n",
        "                roc_output_value = roc_auc_score(truth_values, prediction_values)\n",
        "                final_bias_dictionary[each_model][each_method][each_community] = roc_output_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2000d34",
      "metadata": {
        "id": "e2000d34"
      },
      "outputs": [],
      "source": [
        "# Calculate generalized mean of bias scores\n",
        "power_value = -5\n",
        "num_communities = len(community_list)\n",
        "\n",
        "print(\"\\nBias Scores (Generalized Mean):\")\n",
        "print(\"=\" * 50)\n",
        "for each_model in final_bias_dictionary:\n",
        "    for each_method in final_bias_dictionary[each_model]:\n",
        "        temp_value = []\n",
        "        for each_community in final_bias_dictionary[each_model][each_method]:\n",
        "            temp_value.append(pow(final_bias_dictionary[each_model][each_method][each_community], power_value))\n",
        "        if len(temp_value) > 0:\n",
        "            score = pow(np.sum(temp_value) / num_communities, 1 / power_value)\n",
        "            print(f\"{each_model} | {each_method}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed22331",
      "metadata": {
        "id": "2ed22331"
      },
      "source": [
        "---\n",
        "\n",
        "# Calculate Explainability\n",
        "\n",
        "Based on: DeYoung et al. (2020) - \"ERASER: A Benchmark to Evaluate Rationalized NLP Models\"\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c76dd4",
      "metadata": {
        "id": "88c76dd4"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import more_itertools as mit\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9aa0c6",
      "metadata": {
        "id": "9e9aa0c6"
      },
      "outputs": [],
      "source": [
        "# Import preprocessing utilities\n",
        "from Preprocess.dataCollect import get_annotated_data\n",
        "from Preprocess.spanMatcher import returnMask\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57ef8972",
      "metadata": {
        "id": "57ef8972"
      },
      "outputs": [],
      "source": [
        "# Load 3-class dataset for explainability\n",
        "dict_data_folder = {\n",
        "    '2': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes_two.npy'},\n",
        "    '3': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "params = {}\n",
        "params['num_classes'] = 3  # hatespeech, offensive, normal\n",
        "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "data_all_labelled = get_annotated_data(params)\n",
        "print(f\"Loaded {len(data_all_labelled)} samples for explainability evaluation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eddc527e",
      "metadata": {
        "id": "eddc527e"
      },
      "outputs": [],
      "source": [
        "# Configure tokenization parameters\n",
        "params_data = {\n",
        "    'include_special': False,\n",
        "    'bert_tokens': False,  # Set True for BERT models\n",
        "    'type_attention': 'softmax',\n",
        "    'set_decay': 0.1,\n",
        "    'majority': 2,\n",
        "    'max_length': 128,\n",
        "    'variance': 10,\n",
        "    'window': 4,\n",
        "    'alpha': 0.5,\n",
        "    'p_value': 0.8,\n",
        "    'method': 'additive',\n",
        "    'decay': False,\n",
        "    'normalized': False,\n",
        "    'not_recollect': True,\n",
        "}\n",
        "\n",
        "# Initialize tokenizer\n",
        "if params_data['bert_tokens']:\n",
        "    print('Loading BERT tokenizer...')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
        "else:\n",
        "    print('Using standard tokenizer...')\n",
        "    tokenizer = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ce975f4",
      "metadata": {
        "id": "9ce975f4"
      },
      "outputs": [],
      "source": [
        "def get_training_data(data):\n",
        "    \"\"\"Load dataset and extract token-wise rationales.\"\"\"\n",
        "    final_output = []\n",
        "    print(f'Processing {len(data)} samples...')\n",
        "\n",
        "    for index, row in tqdm(data.iterrows(), total=len(data)):\n",
        "        annotation = row['final_label']\n",
        "        post_id = row['post_id']\n",
        "        annotation_list = [row['label1'], row['label2'], row['label3']]\n",
        "\n",
        "        if annotation != 'undecided':\n",
        "            tokens_all, attention_masks = returnMask(row, params_data, tokenizer)\n",
        "            final_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])\n",
        "\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c18b4ce2",
      "metadata": {
        "id": "c18b4ce2"
      },
      "outputs": [],
      "source": [
        "# Process training data\n",
        "training_data = get_training_data(data_all_labelled)\n",
        "print(f\"Processed {len(training_data)} valid samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71bb9b84",
      "metadata": {
        "id": "71bb9b84"
      },
      "outputs": [],
      "source": [
        "def find_ranges(iterable):\n",
        "    \"\"\"Yield ranges of consecutive numbers.\"\"\"\n",
        "    for group in mit.consecutive_groups(iterable):\n",
        "        group = list(group)\n",
        "        if len(group) == 1:\n",
        "            yield group[0]\n",
        "        else:\n",
        "            yield group[0], group[-1]\n",
        "\n",
        "\n",
        "def get_evidence(post_id, anno_text, explanations):\n",
        "    \"\"\"Convert explanations to ERASER evidence format.\"\"\"\n",
        "    output = []\n",
        "    indexes = sorted([i for i, each in enumerate(explanations) if each == 1])\n",
        "    span_list = list(find_ranges(indexes))\n",
        "\n",
        "    for each in span_list:\n",
        "        if isinstance(each, int):\n",
        "            start, end = each, each + 1\n",
        "        elif len(each) == 2:\n",
        "            start, end = each[0], each[1] + 1\n",
        "        else:\n",
        "            print('Error in span processing')\n",
        "            continue\n",
        "\n",
        "        output.append({\n",
        "            \"docid\": post_id,\n",
        "            \"end_sentence\": -1,\n",
        "            \"end_token\": end,\n",
        "            \"start_sentence\": -1,\n",
        "            \"start_token\": start,\n",
        "            \"text\": ' '.join([str(x) for x in anno_text[start:end]])\n",
        "        })\n",
        "    return output\n",
        "\n",
        "\n",
        "def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):\n",
        "    \"\"\"Convert dataset to ERASER benchmark format.\"\"\"\n",
        "    final_output = []\n",
        "\n",
        "    if save_split:\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        os.makedirs(os.path.join(save_path, 'docs'), exist_ok=True)\n",
        "        train_fp = open(os.path.join(save_path, 'train.jsonl'), 'w')\n",
        "        val_fp = open(os.path.join(save_path, 'val.jsonl'), 'w')\n",
        "        test_fp = open(os.path.join(save_path, 'test.jsonl'), 'w')\n",
        "\n",
        "    for eachrow in dataset:\n",
        "        post_id = eachrow[0]\n",
        "        post_class = eachrow[1]\n",
        "        anno_text_list = eachrow[2]\n",
        "\n",
        "        if post_class == 'normal':\n",
        "            continue\n",
        "\n",
        "        explanations = [list(each_explain) for each_explain in eachrow[3]]\n",
        "\n",
        "        # Union of explanations from all annotators\n",
        "        if method == 'union':\n",
        "            final_explanation = [int(any(each)) for each in zip(*explanations)]\n",
        "\n",
        "        temp = {\n",
        "            'annotation_id': post_id,\n",
        "            'classification': post_class,\n",
        "            'evidences': [get_evidence(post_id, list(anno_text_list), final_explanation)],\n",
        "            'query': \"What is the class?\",\n",
        "            'query_type': None\n",
        "        }\n",
        "        final_output.append(temp)\n",
        "\n",
        "        if save_split:\n",
        "            # Save document\n",
        "            with open(os.path.join(save_path, 'docs', post_id), 'w') as fp:\n",
        "                fp.write(' '.join([str(x) for x in list(anno_text_list)]))\n",
        "\n",
        "            # Save to appropriate split\n",
        "            if post_id in id_division['train']:\n",
        "                train_fp.write(json.dumps(temp) + '\\n')\n",
        "            elif post_id in id_division['val']:\n",
        "                val_fp.write(json.dumps(temp) + '\\n')\n",
        "            elif post_id in id_division['test']:\n",
        "                test_fp.write(json.dumps(temp) + '\\n')\n",
        "\n",
        "    if save_split:\n",
        "        train_fp.close()\n",
        "        val_fp.close()\n",
        "        test_fp.close()\n",
        "\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e8013a1",
      "metadata": {
        "id": "0e8013a1"
      },
      "outputs": [],
      "source": [
        "# Load data splits\n",
        "with open('./Data/post_id_divisions.json') as fp:\n",
        "    id_division = json.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80332ce4",
      "metadata": {
        "id": "80332ce4"
      },
      "outputs": [],
      "source": [
        "# Create evaluation directory\n",
        "os.makedirs('./Data/Evaluation/Model_Eval', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d0e7d11",
      "metadata": {
        "id": "5d0e7d11"
      },
      "outputs": [],
      "source": [
        "# Convert to ERASER format\n",
        "method = 'union'\n",
        "save_split = True\n",
        "save_path = './Data/Evaluation/Model_Eval/'\n",
        "\n",
        "output_eraser = convert_to_eraser_format(training_data, method, save_split, save_path, id_division)\n",
        "print(f\"Converted {len(output_eraser)} samples to ERASER format\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2f7035",
      "metadata": {
        "id": "0b2f7035"
      },
      "outputs": [],
      "source": [
        "# List generated files\n",
        "!ls Data/Evaluation/Model_Eval/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c70dfea6",
      "metadata": {
        "id": "c70dfea6"
      },
      "outputs": [],
      "source": [
        "# Run ERASER metrics\n",
        "explanation_file = './explanations_dicts/bestModel_birnnscrat_100_explanation_top5.json'\n",
        "if os.path.exists(explanation_file):\n",
        "    !cd eraserbenchmark && PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py \\\n",
        "        --split test \\\n",
        "        --data_dir ../Data/Evaluation/Model_Eval \\\n",
        "        --results ../explanations_dicts/bestModel_birnnscrat_100_explanation_top5.json \\\n",
        "        --score_file ../model_explain_output.json\n",
        "else:\n",
        "    print(f\"Explanation file not found: {explanation_file}\")\n",
        "    print(\"Run testing_with_rational.py first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2923617",
      "metadata": {
        "id": "e2923617"
      },
      "outputs": [],
      "source": [
        "# Print explainability results\n",
        "output_file = './model_explain_output.json'\n",
        "if os.path.exists(output_file):\n",
        "    with open(output_file) as fp:\n",
        "        output_data = json.load(fp)\n",
        "\n",
        "    print('\\n' + '=' * 50)\n",
        "    print('EXPLAINABILITY RESULTS')\n",
        "    print('=' * 50)\n",
        "\n",
        "    print('\\nPlausibility:')\n",
        "    print(f\"  IOU F1:   {output_data['iou_scores'][0]['macro']['f1']:.4f}\")\n",
        "    print(f\"  Token F1: {output_data['token_prf']['instance_macro']['f1']:.4f}\")\n",
        "    print(f\"  AUPRC:    {output_data['token_soft_metrics']['auprc']:.4f}\")\n",
        "\n",
        "    print('\\nFaithfulness:')\n",
        "    print(f\"  Comprehensiveness: {output_data['classification_scores']['comprehensiveness']:.4f}\")\n",
        "    print(f\"  Sufficiency:       {output_data['classification_scores']['sufficiency']:.4f}\")\n",
        "else:\n",
        "    print(f\"Output file not found: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98a912fa",
      "metadata": {
        "id": "98a912fa"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. **Model Training**: Training BiRNN-SCRAT model for hate speech detection\n",
        "2. **Bias Evaluation**: Computing subgroup, BPSN, and BNSP bias metrics\n",
        "3. **Explainability Evaluation**: Computing plausibility and faithfulness metrics\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}