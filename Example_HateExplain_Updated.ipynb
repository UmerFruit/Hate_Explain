{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b64eb58",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630f3f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.2' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "import os\n",
    "os.makedirs('Saved', exist_ok=True)\n",
    "os.makedirs('explanations_dicts', exist_ok=True)\n",
    "print(\"Directories created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e725d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this if not already installed)\n",
    "!pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f2403",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare GloVe Embeddings\n",
    "\n",
    "**Note:** This step is only required once. Skip if you already have the word2vec.model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GloVe embeddings (only run if needed)\n",
    "# !wget http://nlp.stanford.edu/data/glove.42B.300d.zip -P Data/\n",
    "# !unzip Data/glove.42B.300d.zip -d Data/\n",
    "# !rm Data/glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GloVe to Word2Vec format (only run if needed)\n",
    "# from gensim.models import KeyedVectors\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# # Convert GloVe format to Word2Vec format\n",
    "# glove2word2vec('Data/glove.42B.300d.txt', 'Data/glove.42B.300d_w2v.txt')\n",
    "\n",
    "# # Load and save in gensim format\n",
    "# word2vecmodel1 = KeyedVectors.load_word2vec_format('Data/glove.42B.300d_w2v.txt', binary=False)\n",
    "# word2vecmodel1.save(\"Data/word2vec.model\")\n",
    "\n",
    "# # Clean up intermediate files\n",
    "# import gc\n",
    "# del word2vecmodel1\n",
    "# gc.collect()\n",
    "\n",
    "# # Remove large text files\n",
    "# import os\n",
    "# os.remove('Data/glove.42B.300d.txt')\n",
    "# os.remove('Data/glove.42B.300d_w2v.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938582a",
   "metadata": {},
   "source": [
    "## 3. Import Dependencies and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training module\n",
    "from manual_training_inference import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9079f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model parameters from JSON configuration\n",
    "import json\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "path_file = 'best_model_json/bestModel_birnnscrat.json'\n",
    "with open(path_file, mode='r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Convert string values to appropriate types\n",
    "for key in params:\n",
    "    if params[key] == 'True':\n",
    "        params[key] = True\n",
    "    elif params[key] == 'False':\n",
    "        params[key] = False\n",
    "    if key in ['batch_size', 'num_classes', 'hidden_size', 'supervised_layer_pos', \n",
    "               'num_supervised_heads', 'random_seed', 'max_length']:\n",
    "        if params[key] != 'N/A':\n",
    "            params[key] = int(params[key])\n",
    "    if (key == 'weights') and (params['auto_weights'] == False):\n",
    "        params[key] = ast.literal_eval(params[key])\n",
    "\n",
    "# Configure for local execution\n",
    "params['logging'] = 'local'\n",
    "params['device'] = 'cpu'  # Change to 'cuda' if GPU is available\n",
    "params['best_params'] = False\n",
    "\n",
    "# Setup device\n",
    "if torch.cuda.is_available() and params['device'] == 'cuda':\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('Using CPU for training.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data folder configuration\n",
    "dict_data_folder = {\n",
    "    '2': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes_two.npy'},\n",
    "    '3': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes.npy'}\n",
    "}\n",
    "\n",
    "# Configure training parameters\n",
    "params['variance'] = 1\n",
    "params['epochs'] = 5  # Reduce for faster testing\n",
    "params['to_save'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e4a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with 2 classes (toxic vs non-toxic)\n",
    "params['num_classes'] = 2\n",
    "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "if params['num_classes'] == 2 and params['auto_weights'] == False:\n",
    "    params['weights'] = [1.0, 1.0]\n",
    "\n",
    "print(f\"Training {params['num_classes']}-class model...\")\n",
    "train_model(params, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0fc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with 3 classes (hatespeech, offensive, normal)\n",
    "params['num_classes'] = 3\n",
    "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "if params['num_classes'] == 2 and params['auto_weights'] == False:\n",
    "    params['weights'] = [1.0, 1.0]\n",
    "\n",
    "print(f\"Training {params['num_classes']}-class model...\")\n",
    "train_model(params, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6620763",
   "metadata": {},
   "source": [
    "## 4. Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run testing scripts\n",
    "!python testing_with_rational.py birnn_scrat 100\n",
    "!python testing_for_bias.py birnn_scrat 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb37bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check generated explanation files\n",
    "!ls explanations_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805472e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bias Calculation\n",
    "\n",
    "Based on: Borkan et al. (2019) - \"Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for bias calculation\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae89aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data collection utilities\n",
    "from Preprocess.dataCollect import get_annotated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7497b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loading for 2-class (toxic/non-toxic)\n",
    "dict_data_folder = {\n",
    "    '2': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes_two.npy'},\n",
    "    '3': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes.npy'}\n",
    "}\n",
    "\n",
    "params = {}\n",
    "params['num_classes'] = 2  # toxic vs non-toxic\n",
    "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "# Load the annotated dataset\n",
    "data_all_labelled = get_annotated_data(params)\n",
    "print(f\"Loaded {len(data_all_labelled)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddf105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "data_all_labelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_information(dataset):\n",
    "    \"\"\"Extract target community based on majority voting among annotators.\"\"\"\n",
    "    final_target_output = defaultdict(list)\n",
    "    all_communities_selected = []\n",
    "    \n",
    "    for each in dataset.iterrows():\n",
    "        # Combine all target communities from 3 annotators\n",
    "        all_targets = each[1]['target1'] + each[1]['target2'] + each[1]['target3']\n",
    "        community_dict = dict(Counter(all_targets))\n",
    "        \n",
    "        # Select communities mentioned by at least 2 annotators\n",
    "        for key in community_dict:\n",
    "            if community_dict[key] > 1:\n",
    "                final_target_output[each[1]['post_id']].append(key)\n",
    "                all_communities_selected.append(key)\n",
    "        \n",
    "        # If no majority, mark as 'None'\n",
    "        if each[1]['post_id'] not in final_target_output:\n",
    "            final_target_output[each[1]['post_id']].append('None')\n",
    "            all_communities_selected.append('None')\n",
    "\n",
    "    return final_target_output, all_communities_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810386dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate target information\n",
    "target_information, all_communities_selected = generate_target_information(data_all_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a7c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 communities for bias calculation\n",
    "community_count_dict = Counter(all_communities_selected)\n",
    "\n",
    "# Remove 'None' and 'Other' from consideration\n",
    "community_count_dict.pop('None', None)\n",
    "community_count_dict.pop('Other', None)\n",
    "\n",
    "# Select top 10 communities\n",
    "list_selected_community = [community for community, value in community_count_dict.most_common(10)]\n",
    "print(f\"Top 10 communities: {list_selected_community}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c85f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter target information to only include top 10 communities\n",
    "final_target_information = {}\n",
    "for each in target_information:\n",
    "    temp = list(set(target_information[each]) & set(list_selected_community))\n",
    "    if len(temp) == 0:\n",
    "        final_target_information[each] = None\n",
    "    else:\n",
    "        final_target_information[each] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add target category column to dataset\n",
    "data_all_labelled['final_target_category'] = data_all_labelled['post_id'].map(final_target_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test split IDs and filter data\n",
    "with open('./Data/post_id_divisions.json', 'r') as fp:\n",
    "    post_id_dict = json.load(fp)\n",
    "\n",
    "data_all_labelled_bias = data_all_labelled[data_all_labelled['post_id'].isin(post_id_dict['test'])]\n",
    "print(f\"Test samples for bias evaluation: {len(data_all_labelled_bias)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Bias score file mapping for the trained model\n",
    "bias_score_file_mapping = {\n",
    "    'BiRNN-Attn': 'bestModel_birnnscrat_bias.json',\n",
    "}\n",
    "\n",
    "parent_path = './explanations_dicts/'\n",
    "method_list = ['subgroup', 'bpsn', 'bnsp']\n",
    "community_list = list(list_selected_community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757024c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_score(label_name, label_dict):\n",
    "    \"\"\"Convert classification to toxicity score [0-1].\"\"\"\n",
    "    if label_name == 'non-toxic':\n",
    "        return 1 - label_dict[label_name]\n",
    "    else:\n",
    "        return label_dict[label_name]\n",
    "\n",
    "\n",
    "def bias_evaluation_metric(dataset, method, community):\n",
    "    \"\"\"Divide IDs into positive/negative based on bias evaluation method.\"\"\"\n",
    "    positive_ids = []\n",
    "    negative_ids = []\n",
    "    \n",
    "    for eachrow in dataset.iterrows():\n",
    "        if eachrow[1]['final_target_category'] is None:\n",
    "            continue\n",
    "            \n",
    "        is_community = community in eachrow[1]['final_target_category']\n",
    "        is_toxic = eachrow[1]['final_label'] != 'non-toxic'\n",
    "        \n",
    "        if method == 'subgroup':\n",
    "            if is_community:\n",
    "                if is_toxic:\n",
    "                    positive_ids.append(eachrow[1]['post_id'])\n",
    "                else:\n",
    "                    negative_ids.append(eachrow[1]['post_id'])\n",
    "        elif method == 'bpsn':\n",
    "            if is_community and not is_toxic:\n",
    "                negative_ids.append(eachrow[1]['post_id'])\n",
    "            elif not is_community and is_toxic:\n",
    "                positive_ids.append(eachrow[1]['post_id'])\n",
    "        elif method == 'bnsp':\n",
    "            if is_community and is_toxic:\n",
    "                positive_ids.append(eachrow[1]['post_id'])\n",
    "            elif not is_community and not is_toxic:\n",
    "                negative_ids.append(eachrow[1]['post_id'])\n",
    "        else:\n",
    "            print('Incorrect method selected!')\n",
    "                \n",
    "    return {'positiveID': positive_ids, 'negativeID': negative_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bias scores\n",
    "final_bias_dictionary = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for each_model in tqdm(bias_score_file_mapping, desc=\"Processing models\"):\n",
    "    total_data = {}\n",
    "    filepath = parent_path + bias_score_file_mapping[each_model]\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: {filepath} not found. Run testing scripts first.\")\n",
    "        continue\n",
    "    \n",
    "    with open(filepath) as fp:\n",
    "        for line in fp:\n",
    "            data = json.loads(line)\n",
    "            total_data[data['annotation_id']] = data\n",
    "    \n",
    "    for each_method in method_list:\n",
    "        for each_community in community_list:\n",
    "            community_data = bias_evaluation_metric(data_all_labelled_bias, each_method, each_community)\n",
    "            truth_values = []\n",
    "            prediction_values = []\n",
    "            \n",
    "            label_to_value = {'toxic': 1.0, 'non-toxic': 0.0}\n",
    "            \n",
    "            for each in community_data['positiveID']:\n",
    "                if each in total_data:\n",
    "                    truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
    "                    prediction_values.append(convert_to_score(\n",
    "                        total_data[each]['classification'], \n",
    "                        total_data[each]['classification_scores']\n",
    "                    ))\n",
    "            \n",
    "            for each in community_data['negativeID']:\n",
    "                if each in total_data:\n",
    "                    truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
    "                    prediction_values.append(convert_to_score(\n",
    "                        total_data[each]['classification'],\n",
    "                        total_data[each]['classification_scores']\n",
    "                    ))\n",
    "            \n",
    "            if len(truth_values) > 0 and len(set(truth_values)) > 1:\n",
    "                roc_output_value = roc_auc_score(truth_values, prediction_values)\n",
    "                final_bias_dictionary[each_model][each_method][each_community] = roc_output_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2000d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate generalized mean of bias scores\n",
    "power_value = -5\n",
    "num_communities = len(community_list)\n",
    "\n",
    "print(\"\\nBias Scores (Generalized Mean):\")\n",
    "print(\"=\" * 50)\n",
    "for each_model in final_bias_dictionary:\n",
    "    for each_method in final_bias_dictionary[each_model]:\n",
    "        temp_value = []\n",
    "        for each_community in final_bias_dictionary[each_model][each_method]:\n",
    "            temp_value.append(pow(final_bias_dictionary[each_model][each_method][each_community], power_value))\n",
    "        if len(temp_value) > 0:\n",
    "            score = pow(np.sum(temp_value) / num_communities, 1 / power_value)\n",
    "            print(f\"{each_model} | {each_method}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed22331",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Calculate Explainability\n",
    "\n",
    "Based on: DeYoung et al. (2020) - \"ERASER: A Benchmark to Evaluate Rationalized NLP Models\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c76dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import more_itertools as mit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9aa0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing utilities\n",
    "from Preprocess.dataCollect import get_annotated_data\n",
    "from Preprocess.spanMatcher import returnMask\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3-class dataset for explainability\n",
    "dict_data_folder = {\n",
    "    '2': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes_two.npy'},\n",
    "    '3': {'data_file': 'Data/dataset.json', 'class_label': 'Data/classes.npy'}\n",
    "}\n",
    "\n",
    "params = {}\n",
    "params['num_classes'] = 3  # hatespeech, offensive, normal\n",
    "params['data_file'] = dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names'] = dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "data_all_labelled = get_annotated_data(params)\n",
    "print(f\"Loaded {len(data_all_labelled)} samples for explainability evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure tokenization parameters\n",
    "params_data = {\n",
    "    'include_special': False,\n",
    "    'bert_tokens': False,  # Set True for BERT models\n",
    "    'type_attention': 'softmax',\n",
    "    'set_decay': 0.1,\n",
    "    'majority': 2,\n",
    "    'max_length': 128,\n",
    "    'variance': 10,\n",
    "    'window': 4,\n",
    "    'alpha': 0.5,\n",
    "    'p_value': 0.8,\n",
    "    'method': 'additive',\n",
    "    'decay': False,\n",
    "    'normalized': False,\n",
    "    'not_recollect': True,\n",
    "}\n",
    "\n",
    "# Initialize tokenizer\n",
    "if params_data['bert_tokens']:\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
    "else:\n",
    "    print('Using standard tokenizer...')\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce975f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(data):\n",
    "    \"\"\"Load dataset and extract token-wise rationales.\"\"\"\n",
    "    final_output = []\n",
    "    print(f'Processing {len(data)} samples...')\n",
    "    \n",
    "    for index, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        annotation = row['final_label']\n",
    "        post_id = row['post_id']\n",
    "        annotation_list = [row['label1'], row['label2'], row['label3']]\n",
    "        \n",
    "        if annotation != 'undecided':\n",
    "            tokens_all, attention_masks = returnMask(row, params_data, tokenizer)\n",
    "            final_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b4ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training data\n",
    "training_data = get_training_data(data_all_labelled)\n",
    "print(f\"Processed {len(training_data)} valid samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ranges(iterable):\n",
    "    \"\"\"Yield ranges of consecutive numbers.\"\"\"\n",
    "    for group in mit.consecutive_groups(iterable):\n",
    "        group = list(group)\n",
    "        if len(group) == 1:\n",
    "            yield group[0]\n",
    "        else:\n",
    "            yield group[0], group[-1]\n",
    "\n",
    "\n",
    "def get_evidence(post_id, anno_text, explanations):\n",
    "    \"\"\"Convert explanations to ERASER evidence format.\"\"\"\n",
    "    output = []\n",
    "    indexes = sorted([i for i, each in enumerate(explanations) if each == 1])\n",
    "    span_list = list(find_ranges(indexes))\n",
    "    \n",
    "    for each in span_list:\n",
    "        if isinstance(each, int):\n",
    "            start, end = each, each + 1\n",
    "        elif len(each) == 2:\n",
    "            start, end = each[0], each[1] + 1\n",
    "        else:\n",
    "            print('Error in span processing')\n",
    "            continue\n",
    "        \n",
    "        output.append({\n",
    "            \"docid\": post_id,\n",
    "            \"end_sentence\": -1,\n",
    "            \"end_token\": end,\n",
    "            \"start_sentence\": -1,\n",
    "            \"start_token\": start,\n",
    "            \"text\": ' '.join([str(x) for x in anno_text[start:end]])\n",
    "        })\n",
    "    return output\n",
    "\n",
    "\n",
    "def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):\n",
    "    \"\"\"Convert dataset to ERASER benchmark format.\"\"\"\n",
    "    final_output = []\n",
    "    \n",
    "    if save_split:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_path, 'docs'), exist_ok=True)\n",
    "        train_fp = open(os.path.join(save_path, 'train.jsonl'), 'w')\n",
    "        val_fp = open(os.path.join(save_path, 'val.jsonl'), 'w')\n",
    "        test_fp = open(os.path.join(save_path, 'test.jsonl'), 'w')\n",
    "    \n",
    "    for eachrow in dataset:\n",
    "        post_id = eachrow[0]\n",
    "        post_class = eachrow[1]\n",
    "        anno_text_list = eachrow[2]\n",
    "        \n",
    "        if post_class == 'normal':\n",
    "            continue\n",
    "        \n",
    "        explanations = [list(each_explain) for each_explain in eachrow[3]]\n",
    "        \n",
    "        # Union of explanations from all annotators\n",
    "        if method == 'union':\n",
    "            final_explanation = [int(any(each)) for each in zip(*explanations)]\n",
    "        \n",
    "        temp = {\n",
    "            'annotation_id': post_id,\n",
    "            'classification': post_class,\n",
    "            'evidences': [get_evidence(post_id, list(anno_text_list), final_explanation)],\n",
    "            'query': \"What is the class?\",\n",
    "            'query_type': None\n",
    "        }\n",
    "        final_output.append(temp)\n",
    "        \n",
    "        if save_split:\n",
    "            # Save document\n",
    "            with open(os.path.join(save_path, 'docs', post_id), 'w') as fp:\n",
    "                fp.write(' '.join([str(x) for x in list(anno_text_list)]))\n",
    "            \n",
    "            # Save to appropriate split\n",
    "            if post_id in id_division['train']:\n",
    "                train_fp.write(json.dumps(temp) + '\\n')\n",
    "            elif post_id in id_division['val']:\n",
    "                val_fp.write(json.dumps(temp) + '\\n')\n",
    "            elif post_id in id_division['test']:\n",
    "                test_fp.write(json.dumps(temp) + '\\n')\n",
    "    \n",
    "    if save_split:\n",
    "        train_fp.close()\n",
    "        val_fp.close()\n",
    "        test_fp.close()\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8013a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data splits\n",
    "with open('./Data/post_id_divisions.json') as fp:\n",
    "    id_division = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80332ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation directory\n",
    "os.makedirs('./Data/Evaluation/Model_Eval', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ERASER format\n",
    "method = 'union'\n",
    "save_split = True\n",
    "save_path = './Data/Evaluation/Model_Eval/'\n",
    "\n",
    "output_eraser = convert_to_eraser_format(training_data, method, save_split, save_path, id_division)\n",
    "print(f\"Converted {len(output_eraser)} samples to ERASER format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f7035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generated files\n",
    "!ls Data/Evaluation/Model_Eval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70dfea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ERASER metrics (from eraserbenchmark directory)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if explanation file exists\n",
    "explanation_file = './explanations_dicts/bestModel_birnnscrat_100_explanation_top5.json'\n",
    "if os.path.exists(explanation_file):\n",
    "    cmd = (\n",
    "        f\"cd eraserbenchmark && \"\n",
    "        f\"PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py \"\n",
    "        f\"--split test \"\n",
    "        f\"--data_dir ../Data/Evaluation/Model_Eval \"\n",
    "        f\"--results ../{explanation_file} \"\n",
    "        f\"--score_file ../model_explain_output.json\"\n",
    "    )\n",
    "    !{cmd}\n",
    "else:\n",
    "    print(f\"Explanation file not found: {explanation_file}\")\n",
    "    print(\"Run testing_with_rational.py first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2923617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print explainability results\n",
    "output_file = './model_explain_output.json'\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file) as fp:\n",
    "        output_data = json.load(fp)\n",
    "    \n",
    "    print('\\n' + '=' * 50)\n",
    "    print('EXPLAINABILITY RESULTS')\n",
    "    print('=' * 50)\n",
    "    \n",
    "    print('\\nPlausibility:')\n",
    "    print(f\"  IOU F1:   {output_data['iou_scores'][0]['macro']['f1']:.4f}\")\n",
    "    print(f\"  Token F1: {output_data['token_prf']['instance_macro']['f1']:.4f}\")\n",
    "    print(f\"  AUPRC:    {output_data['token_soft_metrics']['auprc']:.4f}\")\n",
    "    \n",
    "    print('\\nFaithfulness:')\n",
    "    print(f\"  Comprehensiveness: {output_data['classification_scores']['comprehensiveness']:.4f}\")\n",
    "    print(f\"  Sufficiency:       {output_data['classification_scores']['sufficiency']:.4f}\")\n",
    "else:\n",
    "    print(f\"Output file not found: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a912fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Model Training**: Training BiRNN-SCRAT model for hate speech detection\n",
    "2. **Bias Evaluation**: Computing subgroup, BPSN, and BNSP bias metrics\n",
    "3. **Explainability Evaluation**: Computing plausibility and faithfulness metrics\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
